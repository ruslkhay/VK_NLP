{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10bf3438",
   "metadata": {},
   "source": [
    "# Ассистент 1 - LM на основе n-грамм"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de848df",
   "metadata": {},
   "source": [
    "Перед вами - первое дополнительное задание повышенной сложности, в рамках которого вам предстоит начать разработку телеграм-бота с генеративными моделями.\n",
    "\n",
    "Цель данного ноутбука - помочь влиться в разработку ассистента. В данном ноутбуке написан код для \"обучения\" LM на основе n-грамм, для генерации с помощью нее текста, а также сохранение и загрузка модели и токенизатора.\n",
    "\n",
    "Относитесь к данному заданию максимально творчески - любую часть кода можно менять под ваши нужды и желания, можно оптимизировать, добавлять методы генерации, использовать любые данные, обучать сколь угодно \"большую\" модель. \n",
    "\n",
    "При этом вам стоит быть готовыми со всеми техническими проблеми справляться самому - именно так обычно происходит в реальной жизни в реальных проектах :) \n",
    "\n",
    "Поэтому отдельно подчеркну:\n",
    "* если что-то сломалось после ваших изменений - подразумевается, что вы сами найдете проблему и исправите\n",
    "* если вы ничего не трогали, но что-то не работает у нас - подразумевается, что вы сами найдете проблему и исправите :) \n",
    "\n",
    "Главный критерий выполнености данного задания - телеграм-бот, генерирующий текст и использующий обозначенный в задании подход (в случае данного ноутбука - n-граммная модель в любой ее реализации)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecb8bd4",
   "metadata": {},
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ca60e8",
   "metadata": {},
   "source": [
    "Для обучения качественной модели вам потребуются датасеты. В ноутбуке составлен маленький игрушечный датасет, вам для улучшения качества потребуется данные в большем количестве и более качественные, а также другие параметры модели и генерации (например, размер контекста побольше). \n",
    "\n",
    "С нормальным датасетом и правильными параметрами даже такой простой моделью можно добиться адекватного качества генерации текста (возможно не очень человечный, но вполне связный текст).\n",
    "\n",
    "Датасеты можно найти и выбрать тут (желательно на русском, вам так будет понятней качество и в целом полезней):\n",
    "https://huggingface.co/datasets\n",
    "  \n",
    "Можете найти наиболее интересный для себя датасет (можете сделать модель как смешной, так и полезной), либо выбрать любой из этих датасетов\n",
    "* https://huggingface.co/datasets/Den4ikAI/russian_dialogues\n",
    "* https://huggingface.co/datasets/Georgii/russianPoetry\n",
    "* https://huggingface.co/datasets/IgorVolochay/russian_jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c6c33a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T14:50:17.414585Z",
     "iopub.status.busy": "2024-02-21T14:50:17.413838Z",
     "iopub.status.idle": "2024-02-21T14:50:37.187697Z",
     "shell.execute_reply": "2024-02-21T14:50:37.186316Z",
     "shell.execute_reply.started": "2024-02-21T14:50:17.414553Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle \n",
    "from itertools import chain\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "from typing import List, Dict, Optional, Iterable, Tuple\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df809daa",
   "metadata": {},
   "source": [
    "Токенизатор разбивает текст на слова. Можно попробовать другие способы токенизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90160389",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:11.789756Z",
     "iopub.status.busy": "2024-02-21T15:03:11.788899Z",
     "iopub.status.idle": "2024-02-21T15:03:11.808584Z",
     "shell.execute_reply": "2024-02-21T15:03:11.807519Z",
     "shell.execute_reply.started": "2024-02-21T15:03:11.789725Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self,\n",
    "                 token_pattern: str = '\\w+|[\\!\\?\\,\\.\\-\\:]',\n",
    "                 eos_token: str = '<EOS>',\n",
    "                 pad_token: str = '<PAD>',\n",
    "                 unk_token: str = '<UNK>'):\n",
    "        self.token_pattern = token_pattern\n",
    "        self.eos_token = eos_token\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        \n",
    "        self.special_tokens = [self.eos_token, self.pad_token, self.unk_token]\n",
    "        self.vocab = None\n",
    "        self.inverse_vocab = None\n",
    "    \n",
    "    def text_preprocess(self, input_text: str) -> str:\n",
    "        \"\"\" Предобрабатываем один текст \"\"\"\n",
    "        # input_text = ... # приведение к нижнему регистру\n",
    "        input_text = input_text.lower()\n",
    "        input_text = re.sub('\\s+', ' ', input_text) # унифицируем пробелы\n",
    "        input_text = input_text.strip()\n",
    "        return input_text\n",
    "    \n",
    "    def build_vocab(self, corpus: List[str]) -> None:\n",
    "        assert len(corpus)\n",
    "        all_tokens = set()\n",
    "        for text in corpus:\n",
    "            all_tokens |= set(self._tokenize(text, append_eos_token=False))\n",
    "        self.vocab = {elem: ind for ind, elem in enumerate(all_tokens)}\n",
    "        special_tokens = [self.eos_token, self.unk_token, self.pad_token]\n",
    "        for token in special_tokens:\n",
    "            self.vocab[token] = len(self.vocab)\n",
    "        self.inverse_vocab = {ind: elem for elem, ind in self.vocab.items()}\n",
    "        return self\n",
    "        \n",
    "    def _tokenize(self, text: str, append_eos_token: bool = True) -> List[str]:\n",
    "        text = self.text_preprocess(text)\n",
    "        tokens = re.findall(self.token_pattern, text)\n",
    "        if append_eos_token:\n",
    "            tokens.append(self.eos_token)\n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, text: str, append_eos_token: bool = True) -> List[str]:\n",
    "        \"\"\" Токенизируем текст \"\"\"\n",
    "        tokens = self._tokenize(text, append_eos_token)\n",
    "        ids = [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, input_ids: Iterable[int], remove_special_tokens: bool = False) -> str:\n",
    "        assert len(input_ids)\n",
    "        assert max(input_ids) < len(self.vocab) and min(input_ids) >= 0\n",
    "        tokens = []\n",
    "        for ind in input_ids:\n",
    "            token = self.inverse_vocab[ind]\n",
    "            if remove_special_tokens and token in self.special_tokens:\n",
    "                continue\n",
    "            tokens.append(token)\n",
    "        text = ' '.join( tokens )\n",
    "        return text\n",
    "    \n",
    "    def save(self, path: str) -> bool:\n",
    "        data = {\n",
    "            'token_pattern': self.token_pattern,\n",
    "            'eos_token': self.eos_token,\n",
    "            'pad_token': self.pad_token,\n",
    "            'unk_token': self.unk_token,\n",
    "            'special_tokens': self.special_tokens,\n",
    "            'vocab': self.vocab,\n",
    "            'inverse_vocab': self.inverse_vocab,\n",
    "        }\n",
    "        \n",
    "        with open(path, 'wb') as fout:\n",
    "            pickle.dump(data, fout)\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    def load(self, path: str) -> bool:\n",
    "        with open(path, 'rb') as fin:\n",
    "            data = pickle.load(fin)\n",
    "            \n",
    "        self.token_pattern = data['token_pattern']\n",
    "        self.eos_token = data['eos_token']\n",
    "        self.pad_token = data['pad_token']\n",
    "        self.unk_token = data['unk_token']\n",
    "        self.special_tokens = data['special_tokens']\n",
    "        self.vocab = data['vocab']\n",
    "        self.inverse_vocab = data['inverse_vocab']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6999f620",
   "metadata": {},
   "source": [
    "Класс для задания параметров генерации, так удобней писать логику для валидации параметров и разные другие доп методы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f14d758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationConfig:\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Тут можно задать любые параметры и их значения по умолчанию\n",
    "        Значения для стратегии декодирования decoding_strategy: ['max', 'top-p']\n",
    "        \"\"\"\n",
    "        self.temperature = kwargs.pop(\"temperature\", 1.0)\n",
    "        self.max_tokens = kwargs.pop(\"max_tokens\", 32)\n",
    "        self.sample_top_p = kwargs.pop(\"sample_top_p\", 0.9)\n",
    "        self.decoding_strategy = kwargs.pop(\"decoding_strategy\", 'max')\n",
    "        self.remove_special_tokens = kwargs.pop(\"remove_special_tokens\", False)\n",
    "        self.validate()\n",
    "        \n",
    "    def validate(self):\n",
    "        \"\"\" Здесь можно валидировать параметры \"\"\"\n",
    "        if not (1.0 > self.sample_top_p > 0):\n",
    "            raise ValueError('sample_top_p')\n",
    "        if self.decoding_strategy not in ['max', 'top-p']:\n",
    "            raise ValueError('decoding_strategy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd923a4",
   "metadata": {},
   "source": [
    "Сама LM на основе n-грамм. Тут используется сглаживание Лапласа (можно поменять на метод backoff при желании), а также есть ряд параметров, сильно влияющий на качество генерации. Один из параметров генерации - стратегия генерации. \n",
    "\n",
    "Когда мы получили вероятности для следующего токена, мы по этим вероятностям хотим выбрать этот следующий токен.\n",
    "\n",
    "Можно просто семплировать из этого распределения - но тогда есть шанс, что будут очень маловероятные токены.\n",
    "\n",
    "Можно брать самый вероятный токен - но это плохо повлияет на разнообразие и \"человечность\" языка\n",
    "\n",
    "Можно воспользовать подходом top-p - семплировать только из тех токенов, которые наиболее вероятны (их вероятности суммируются в заданный p)\n",
    "\n",
    "Можно проверить, что top-p будет генерировать более интересный текст чем max\n",
    "\n",
    "Также обратите внимание на параметр температуры. В случае top-p и семплирования, чем больше делаешь температуру, тем меньше отличаются друг от друга вероятности (распределение стремится к равномерному, даже если исходное распределение имело вполне себе выраженные максимумы), и текст становится более случайным (и разнообразным)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "462316de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:21.268282Z",
     "iopub.status.busy": "2024-02-21T15:03:21.267567Z",
     "iopub.status.idle": "2024-02-21T15:03:21.292501Z",
     "shell.execute_reply": "2024-02-21T15:03:21.291323Z",
     "shell.execute_reply.started": "2024-02-21T15:03:21.268251Z"
    }
   },
   "outputs": [],
   "source": [
    "class StatLM:\n",
    "    def __init__(self, \n",
    "                 tokenizer: Tokenizer,\n",
    "                 context_size: int = 2,\n",
    "                 alpha: float = 0.1\n",
    "                ):\n",
    "        \n",
    "        assert context_size >= 2\n",
    "        \n",
    "        self.context_size = context_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.n_gramms_stat = defaultdict(int)\n",
    "        self.nx_gramms_stat = defaultdict(int)\n",
    "        \n",
    "    def get_token_by_ind(ind: int) -> str:\n",
    "        return self.tokenizer.vocab.get(ind)\n",
    "    \n",
    "    def get_ind_by_token(token: str) -> int:\n",
    "        return self.tokenizer.inverse_vocab.get(token, self.tokenizer.inverse_vocab[self.unk_token])\n",
    "        \n",
    "    def train(self, train_texts: List[str]):\n",
    "        for sentence in tqdm(train_texts, desc='train lines'):\n",
    "            sentence_ind = self.tokenizer.encode(sentence)\n",
    "            for i in range(len(sentence_ind) - self.context_size):\n",
    "                \n",
    "                seq = tuple(sentence_ind[i: i + self.context_size - 1])\n",
    "                self.n_gramms_stat[seq] += 1\n",
    "                \n",
    "                seq_x = tuple(sentence_ind[i: i + self.context_size])\n",
    "                self.nx_gramms_stat[seq_x] += 1\n",
    "                \n",
    "            seq = tuple(sentence_ind[len(sentence_ind) - self.context_size:])\n",
    "            self.n_gramms_stat[seq] += 1\n",
    "            \n",
    "    def sample_token(self, \n",
    "                     token_distribution: np.ndarray,\n",
    "                     generation_config: GenerationConfig) -> int:\n",
    "        if generation_config.decoding_strategy == 'max':\n",
    "            return token_distribution.argmax()\n",
    "        elif generation_config.decoding_strategy == 'top-p':\n",
    "            token_distribution = sorted(list(zip(token_distribution, np.arange(len(token_distribution)))),\n",
    "                                        reverse=True)\n",
    "            total_proba = 0.0\n",
    "            tokens_to_sample = []\n",
    "            tokens_probas = []\n",
    "            for token_proba, ind in token_distribution:\n",
    "                tokens_to_sample.append(ind)\n",
    "                tokens_probas.append(token_proba)\n",
    "                total_proba += token_proba\n",
    "                if total_proba >= generation_config.sample_top_p:\n",
    "                    break\n",
    "            # для простоты отнормируем вероятности, чтобы суммировались в единицу\n",
    "            tokens_probas = np.array(tokens_probas) / generation_config.temperature\n",
    "            tokens_probas = tokens_probas / tokens_probas.sum()\n",
    "            return np.random.choice(tokens_to_sample, p=tokens_probas)\n",
    "        else:\n",
    "            raise ValueError(f'Unknown decoding strategy: {generation_config.decoding_strategy}')\n",
    "            \n",
    "    def save_stat(self, path: str) -> bool:\n",
    "        stat = {\n",
    "            'n_gramms_stat': self.n_gramms_stat,\n",
    "            'nx_gramms_stat': self.nx_gramms_stat,\n",
    "            'context_size': self.context_size,\n",
    "            'alpha': self.alpha\n",
    "        }\n",
    "        with open(path, 'wb') as fout:\n",
    "            pickle.dump(stat, fout)\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def load_stat(self, path: str) -> bool:\n",
    "        with open(path, 'rb') as fin:\n",
    "            stat = pickle.load(fin)\n",
    "            \n",
    "        self.n_gramms_stat = stat['n_gramms_stat']\n",
    "        self.nx_gramms_stat = stat['nx_gramms_stat']\n",
    "        self.context_size = stat['context_size']\n",
    "        self.alpha = stat['alpha']\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    def get_stat(self) -> Dict[str, Dict]:\n",
    "        \n",
    "        n_token_stat, nx_token_stat = {}, {}\n",
    "        for token_inds, count in self.n_gramms_stat.items():\n",
    "            n_token_stat[self.tokenizer.decode(token_inds)] = count\n",
    "        \n",
    "        for token_inds, count in self.nx_gramms_stat.items():\n",
    "            nx_token_stat[self.tokenizer.decode(token_inds)] = count\n",
    "        \n",
    "        return {\n",
    "            'n gramms stat': self.n_gramms_stat,\n",
    "            'n+1 gramms stat': self.nx_gramms_stat,\n",
    "            'n tokens stat': n_token_stat,\n",
    "            'n+1 tokens stat': nx_token_stat,\n",
    "        }\n",
    "    \n",
    "    def _get_next_token(self, \n",
    "                        tokens: List[int],\n",
    "                        generation_config: GenerationConfig) -> (int, str):\n",
    "        denominator = self.n_gramms_stat.get(tuple(tokens), 0) + self.alpha * len(self.tokenizer.vocab)\n",
    "        numerators = []\n",
    "        for ind in self.tokenizer.inverse_vocab:\n",
    "            numerators.append(self.nx_gramms_stat.get(tuple(tokens + [ind]), 0) + self.alpha)\n",
    "        \n",
    "        token_distribution = np.array(numerators) / denominator\n",
    "        max_proba_ind = self.sample_token(token_distribution, generation_config)\n",
    "        \n",
    "        next_token = self.tokenizer.inverse_vocab[max_proba_ind]\n",
    "        \n",
    "        return max_proba_ind, next_token\n",
    "            \n",
    "    def generate_token(self, \n",
    "                       text: str, \n",
    "                       generation_config: GenerationConfig\n",
    "                      ) -> Dict:\n",
    "        tokens = self.tokenizer.encode(text, append_eos_token=False)\n",
    "        tokens = tokens[-self.context_size + 1:]\n",
    "        \n",
    "        max_proba_ind, next_token = self._get_next_token(tokens, generation_config)\n",
    "        \n",
    "        return {\n",
    "            'next_token': next_token,\n",
    "            'next_token_num': max_proba_ind,\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def generate_text(self, text: str, \n",
    "                      generation_config: GenerationConfig\n",
    "                     ) -> Dict:\n",
    "        \n",
    "        all_tokens = self.tokenizer.encode(text, append_eos_token=False)\n",
    "        tokens = all_tokens[-self.context_size + 1:]\n",
    "        \n",
    "        next_token = None\n",
    "        while next_token != self.tokenizer.eos_token and len(all_tokens) < generation_config.max_tokens:\n",
    "            max_proba_ind, next_token = self._get_next_token(tokens, generation_config)\n",
    "            all_tokens.append(max_proba_ind)\n",
    "            tokens = all_tokens[-self.context_size + 1:]\n",
    "        \n",
    "        new_text = self.tokenizer.decode(all_tokens, generation_config.remove_special_tokens)\n",
    "        \n",
    "        finish_reason = 'max tokens'\n",
    "        if all_tokens[-1] == self.tokenizer.vocab[self.tokenizer.eos_token]:\n",
    "            finish_reason = 'end of text'\n",
    "        \n",
    "        return {\n",
    "            'all_tokens': all_tokens,\n",
    "            'total_text': new_text,\n",
    "            'finish_reason': finish_reason\n",
    "        }\n",
    "    \n",
    "    def generate(self, text: str, generation_config: Dict) -> str:\n",
    "        return self.generate_text(text, generation_config)['total_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f60f19",
   "metadata": {},
   "source": [
    "Эта функция напрямую используется в телеграм боте для получения модели и конфига генерации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cfdad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model():\n",
    "    config = {\n",
    "        'temperature': 1.0,\n",
    "        'max_tokens': 32,\n",
    "        'sample_top_p': 0.9,\n",
    "        'decoding_strategy': 'top-p',\n",
    "    }\n",
    "\n",
    "    stat_lm_path = 'models/stat_lm/stat_lm.pkl'\n",
    "    tokenizer_path = 'models/stat_lm/tokenizer.pkl'\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.load(tokenizer_path)\n",
    "        \n",
    "    stat_lm = StatLM(tokenizer)\n",
    "    stat_lm.load_stat(stat_lm_path)\n",
    "\n",
    "    generation_config = GenerationConfig(temperature=config['temperature'],\n",
    "                                         max_tokens=config['max_tokens'],\n",
    "                                         sample_top_p=config['sample_top_p'],\n",
    "                                         decoding_strategy=config['decoding_strategy'],\n",
    "                                         remove_special_tokens=True)\n",
    "\n",
    "    kwargs = {'generation_config': generation_config}\n",
    "    return stat_lm, kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a85b767",
   "metadata": {},
   "source": [
    "### Обучаем на игрушечных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674d63a",
   "metadata": {},
   "source": [
    "Для демонстрации того, что происходит, возьмем несколько коротких цитат Джейсона Стэтхема отсюда:\n",
    "\n",
    "https://dzen.ru/a/ZRFaGN_gKhX6xTWW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b515d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(temperature = 1.0, max_tokens = 32,\n",
    "                                     sample_top_p = 0.9, decoding_strategy = 'max',\n",
    "                                     remove_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fa25b3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:23.642385Z",
     "iopub.status.busy": "2024-02-21T15:03:23.641563Z",
     "iopub.status.idle": "2024-02-21T15:03:23.647640Z",
     "shell.execute_reply": "2024-02-21T15:03:23.646711Z",
     "shell.execute_reply.started": "2024-02-21T15:03:23.642338Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'Взял нож - режь, взял дошик - ешь.',\n",
    "    'Никогда не сдавайтесь, идите к своей цели! А если будет сложно – сдавайтесь.',\n",
    "    'Запомни: всего одна ошибка – и ты ошибся.',\n",
    "    'В жизни всегда есть две дороги: одна — первая, а другая — вторая.',\n",
    "    'Делай, как надо. Как не надо, не делай.',\n",
    "    'Работа не волк. Никто не волк. Только волк волк.',\n",
    "    'Работа не волк. Работа - ворк. А волк - это ходить.',\n",
    "    'Работа',\n",
    "    ]\n",
    "\n",
    "train_texts = texts[:-1]\n",
    "test_text = texts[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98e92a50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:25.218697Z",
     "iopub.status.busy": "2024-02-21T15:03:25.218225Z",
     "iopub.status.idle": "2024-02-21T15:03:25.223271Z",
     "shell.execute_reply": "2024-02-21T15:03:25.222261Z",
     "shell.execute_reply.started": "2024-02-21T15:03:25.218665Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer().build_vocab(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f61a4e82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:25.398291Z",
     "iopub.status.busy": "2024-02-21T15:03:25.397981Z",
     "iopub.status.idle": "2024-02-21T15:03:25.406154Z",
     "shell.execute_reply": "2024-02-21T15:03:25.405134Z",
     "shell.execute_reply.started": "2024-02-21T15:03:25.398265Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'нож': 0, 'цели': 1, 'сложно': 2, 'никто': 3, 'ты': 4}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(tokenizer.vocab.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ef0948d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:29.936948Z",
     "iopub.status.busy": "2024-02-21T15:03:29.936563Z",
     "iopub.status.idle": "2024-02-21T15:03:29.957657Z",
     "shell.execute_reply": "2024-02-21T15:03:29.956641Z",
     "shell.execute_reply.started": "2024-02-21T15:03:29.936919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3f740de6ea4bfd859abacc314cea77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train lines:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# класс, который позволяем строить и использовать языковую модель на основе n-грамм\n",
    "stat_lm = StatLM(tokenizer, context_size=2, alpha=0.1) # , sample_top_p = None\n",
    "\n",
    "# \"обучаем\" модель - считаем статистики\n",
    "stat_lm.train(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e73f538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! а волк . никто не волк . никто не волк . никто не волк . никто не волк . никто не волк . никто не волк . никто\n"
     ]
    }
   ],
   "source": [
    "print(stat_lm.generate(\"вот такие пироги!\", generation_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d054a1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save('models/stat_lm/tokenizer.pkl')\n",
    "stat_lm.save_stat('models/stat_lm/stat_lm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600bcef7",
   "metadata": {},
   "source": [
    "Тут мы для токенизатора сохраняем только спецтокены и словарь, для модели - параметры и статистики n-грамм и n+1-грамм. Потом в телеграм боте подгружаем именно эти параметры"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbf63a9",
   "metadata": {},
   "source": [
    "Когда обучите модель на большом датасете, советую посмотреть на распределение вероятностей для следующего слова при разных входах"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e659ff6",
   "metadata": {},
   "source": [
    "### смотрим как конструировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ad87959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'дошик - режь одна . взял нож другая как вторая две дороги как ворк . как надо , а другая вторая режь сдавайтесь надо дороги делай взял дошик в режь всегда'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, kwargs = construct_model()\n",
    "\n",
    "model.generate(\"дошик\", **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517748d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
