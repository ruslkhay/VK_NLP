{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymnTEOhN3Ytl"
      },
      "outputs": [],
      "source": [
        "! ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://huggingface.co/openai-community/gpt2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "695xY_Hk3f7E",
        "outputId": "ff6e0ddc-1a77-449f-ba75-5ac5fc89c033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gpt2'...\n",
            "remote: Enumerating objects: 87, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 87 (delta 0), reused 0 (delta 0), pack-reused 84\u001b[K\n",
            "Unpacking objects: 100% (87/87), 1.65 MiB | 6.61 MiB/s, done.\n",
            "Filtering content: 100% (11/11), 5.23 GiB | 48.40 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nrS5HRVw4J4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Загрузка модели**"
      ],
      "metadata": {
        "id": "Fuu1sk6o4Vjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch"
      ],
      "metadata": {
        "id": "1zuU-b6G4MR8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DlqIeaDHpDqq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'"
      ],
      "metadata": {
        "id": "TeKWzYwZ4MUj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mNAyk7RI4nXp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    'gpt2',\n",
        "    torch_dtype=torch.float64,\n",
        ")\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dd5vp1vC4MXa",
        "outputId": "e9a9247e-1d15-4f0b-da81-e31aa8f97114"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_special_tokens({'pad_token': \"<|endoftext|>\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG6Y5VXs4Maj",
        "outputId": "cdbb1baf-f7f8-4636-cb96-adbd0aca8ffa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lDvHL8oq4QBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Задание 1. LayerNorm vs RMSNorm**\n",
        "\n",
        "для примера ниже нужно релизовать подсчет var и вычислить входной вектор в случае LayerNorm и RMSNorm\n",
        "\n",
        "\n",
        "\n",
        "1) LayerNorm:\n",
        "\n",
        "the paper: https://arxiv.org/abs/1607.06450\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xwnnFYLe4dey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1rUfBxhGNGBw98EGWlCq7rOKYUEoyld2i)"
      ],
      "metadata": {
        "id": "YqikWhZwDXkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) RMSNorm:\n",
        "\n",
        "the paper: https://arxiv.org/pdf/1910.07467v1.pdf"
      ],
      "metadata": {
        "id": "EI0bBsLNEx_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1s5Ed7ZRsZ1KvIT6Np-zSiBhckYc1NfWQ)"
      ],
      "metadata": {
        "id": "QY-MMa7xEyD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = torch.Tensor([[1, 2, 3, 4, 5], [2, 3, 4, 5, 6]])"
      ],
      "metadata": {
        "id": "JHximgUQ4QFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_Nez0oT4QI4",
        "outputId": "b2f41b2b-ba77-4021-9c04-809a6e73624d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2., 3., 4., 5.],\n",
              "        [2., 3., 4., 5., 6.]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm:\n",
        "  def __init__(self, eps: float = 1e-5):\n",
        "    self.eps = eps\n",
        "\n",
        "  def __call__(self, input: torch.Tensor):\n",
        "    mean = (input.sum(axis=1) / input.shape[1]).reshape(-1, 1)\n",
        "    var = # your code\n",
        "    return (input - mean) / (torch.sqrt(var + self.eps))"
      ],
      "metadata": {
        "id": "pt8hCAW94m_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_norm = LayerNorm()\n",
        "result = layer_norm(example)\n",
        "result"
      ],
      "metadata": {
        "id": "_ZIO1D8ZFtW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### answer: result[0][0]"
      ],
      "metadata": {
        "id": "c3DAbpG6F5IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K-mo48HLFxxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm:\n",
        "  def __init__(self, eps: float = 1e-5):\n",
        "    self.eps = eps\n",
        "\n",
        "  def __call__(self, input: torch.Tensor):\n",
        "    var = # your code\n",
        "    return input / torch.sqrt(var + self.eps)"
      ],
      "metadata": {
        "id": "9X6bFai14nFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rms_norm = RMSNorm()\n",
        "result = rms_norm(example)\n",
        "result"
      ],
      "metadata": {
        "id": "nylEtfu84nIc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### answer: result[0][0]"
      ],
      "metadata": {
        "id": "97YxTO6r4nLm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U2K1lcVP8ifL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 2. Обучение модели.\n",
        "\n",
        "1) необходимо подобрать такой lr, при котором лосс будет иметь тенденцию к снижению (скользящее срденее которого убывает)\n",
        "\n"
      ],
      "metadata": {
        "id": "X5mQkHoBzl7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from torch import nn\n",
        "import transformers\n",
        "import torch"
      ],
      "metadata": {
        "id": "RnyMTIT-9QUg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZCrNC-GIztzs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sft_df = pd.read_parquet('argilla_df.parquet')"
      ],
      "metadata": {
        "id": "MRgsSN9Fzt2k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from typing import Dict, Sequence, List, Iterator, Tuple\n",
        "\n",
        "IGNORE_INDEX: int = -100\n",
        "pad_token_id = 60000\n",
        "\n",
        "def _tokenize_fn(text: str, max_length: int, tokenizer: transformers.PreTrainedTokenizer) -> List[int]:\n",
        "    return tokenizer(\n",
        "        text,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        add_special_tokens=False,\n",
        "    )['input_ids']\n",
        "\n",
        "class SupervisedDataset(Dataset):\n",
        "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
        "    def __init__(self, dataset, tokenizer: transformers.PreTrainedTokenizer):\n",
        "        super(SupervisedDataset, self).__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        bos_token = tokenizer.bos_token if tokenizer.bos_token is not None else ''\n",
        "        self.sources = [f\"{bos_token}{example[0]}\" for example in dataset]\n",
        "        self.targets = [f\"{example[1]}{tokenizer.eos_token}\" for example in dataset]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.sources)\n",
        "\n",
        "    def padding(self, ids: torch.tensor, max_length: int) -> torch.tensor:\n",
        "        return\n",
        "\n",
        "    def __getitem__(self, i: int) -> Dict[str, List[int]]:\n",
        "        source_ids = _tokenize_fn(self.sources[i], self.tokenizer.model_max_length, self.tokenizer)\n",
        "        target_ids = _tokenize_fn(self.targets[i], self.tokenizer.model_max_length - len(source_ids), self.tokenizer)\n",
        "        input_ids = torch.tensor(source_ids + target_ids)\n",
        "\n",
        "        input_pad_len = self.tokenizer.model_max_length - len(input_ids)\n",
        "        input_ids_pad = torch.tensor(source_ids + target_ids + [self.tokenizer.pad_token_id]*input_pad_len)\n",
        "        input_att_mask = torch.tensor([[1]*len(input_ids) + [0]*input_pad_len])\n",
        "\n",
        "        labels = torch.tensor([IGNORE_INDEX] * len(source_ids) + target_ids + [pad_token_id]*input_pad_len)\n",
        "\n",
        "        return dict(input_ids=input_ids_pad, input_att_mask=input_att_mask,\n",
        "                    labels=labels)\n"
      ],
      "metadata": {
        "id": "Erd0jNdFzt5e"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfd6DMaBGY27",
        "outputId": "14de2c86-e2c2-4089-80d1-40b8bc898780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50256"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F7H9zRS4seR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch = SupervisedDataset(\n",
        "    dataset=sft_df[:200][['prompt', 'original_response']].values,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "QHAG29nMzt8D"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch_loader = DataLoader(\n",
        "    dataset=train_batch,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        ")"
      ],
      "metadata": {
        "id": "JIM0iv5pzt-7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w48CS5UwHY0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr= ### Your value)\n",
        "N_ITERATIONS = 100\n",
        "cur_iteration = 0\n",
        "loss_values = []\n",
        "for x in train_batch_loader:\n",
        "    if cur_iteration == N_ITERATIONS:\n",
        "      break\n",
        "\n",
        "    model.train()\n",
        "    #################\n",
        "    input_tokens = x['input_ids'].to(device)\n",
        "    attention_mask = x['input_att_mask'].to(device)\n",
        "    labels = x['labels'].clone().to(device)\n",
        "    out_logits = model(input_ids=input_tokens, attention_mask=attention_mask).logits\n",
        "\n",
        "    ### не затираем токены начала и конца, чтобы модель училась останавливаться ###\n",
        "    labels[labels == pad_token_id] = -100\n",
        "\n",
        "    loss_value = loss(out_logits.permute(0, 2, 1), labels)\n",
        "    #################\n",
        "    print(f\"Loss value: {loss_value.item()}\")\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    cur_iteration += 1\n",
        "    loss_values.append(loss_value.cpu().detach().numpy())"
      ],
      "metadata": {
        "id": "hs3ZJpxQzlxu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PN58Q5kee6_Q"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2) обратите внимание на то, как выглядит лосс"
      ],
      "metadata": {
        "id": "x4HgHb93MdFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data for plotting\n",
        "t = [i for i in range(len(loss_values))]\n",
        "s = loss_values\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(t, s)\n",
        "\n",
        "ax.set(xlabel='step', ylabel='loss value',\n",
        "       title='Train Loss')\n",
        "ax.grid()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7hijd8mYHgJ6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BkRpXkeYHgMi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WbgbeNAuMHPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MkdJWNZ0MHW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) попробуйте явно прописать задание модели, то есть в качестве промпта задать: Тебе необходимо ответить на вопрос, приведенные ниже: ....\n",
        "\n",
        "вставьте следующий текст перед самим вопросом: \"You must answer the question correctly. Question: \"\n",
        "\n",
        "вставьте следующие текст между самим вопросом и правильным ответом на вопрос: \"\\n###answer: \""
      ],
      "metadata": {
        "id": "PoxX75EbMINU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "isdTn2M4n24d"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from typing import Dict, Sequence, List, Iterator, Tuple\n",
        "\n",
        "IGNORE_INDEX: int = -100\n",
        "pad_token_id = 60000\n",
        "\n",
        "def _tokenize_fn(text: str, max_length: int, tokenizer: transformers.PreTrainedTokenizer) -> List[int]:\n",
        "    return tokenizer(\n",
        "        text,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        add_special_tokens=False,\n",
        "    )['input_ids']\n",
        "\n",
        "class SupervisedDataset(Dataset):\n",
        "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
        "    def __init__(self, dataset, tokenizer: transformers.PreTrainedTokenizer):\n",
        "        super(SupervisedDataset, self).__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        bos_token = tokenizer.bos_token if tokenizer.bos_token is not None else ''\n",
        "        self.sources = [f\"{bos_token}{example[0]}\" for example in dataset]\n",
        "        self.targets = [f\"{example[1]}{tokenizer.eos_token}\" for example in dataset]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.sources)\n",
        "\n",
        "    def padding(self, ids: torch.tensor, max_length: int) -> torch.tensor:\n",
        "        return\n",
        "\n",
        "    def __getitem__(self, i: int) -> Dict[str, List[int]]:\n",
        "        source_ids = _tokenize_fn(self.sources[i], self.tokenizer.model_max_length, self.tokenizer)\n",
        "        target_ids = _tokenize_fn(self.targets[i], self.tokenizer.model_max_length - len(source_ids), self.tokenizer)\n",
        "        input_ids = torch.tensor(source_ids + target_ids)\n",
        "\n",
        "        input_pad_len = self.tokenizer.model_max_length - len(input_ids)\n",
        "        input_ids_pad = torch.tensor(source_ids + target_ids + [self.tokenizer.pad_token_id]*input_pad_len)\n",
        "        input_att_mask = torch.tensor([[1]*len(input_ids) + [0]*input_pad_len])\n",
        "\n",
        "        labels = torch.tensor([IGNORE_INDEX] * len(source_ids) + target_ids + [pad_token_id]*input_pad_len)\n",
        "\n",
        "        return dict(input_ids=input_ids_pad, input_att_mask=input_att_mask,\n",
        "                    labels=labels)\n"
      ],
      "metadata": {
        "id": "1ysMvyaP-uNF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### вставьте следующий текст перед самим вопросом: \"You must answer the question correctly. Question: \"\n",
        "### вставьте следующие текст между самим вопросом и правильным ответом на вопрос: \"\\n###answer: \""
      ],
      "metadata": {
        "id": "7JnQ31zfJ0q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a-S_1aN2J00z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch = SupervisedDataset(\n",
        "    dataset=sft_df[:200][['prompt', 'original_response']].values,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "GE40vy5HpYf5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch_loader = DataLoader(\n",
        "    dataset=train_batch,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        ")"
      ],
      "metadata": {
        "id": "LtGwpF_zqtON"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in train_batch_loader:\n",
        "  break"
      ],
      "metadata": {
        "id": "tYPhU8dvKiM_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### answer: x['input_ids'][0][:10]"
      ],
      "metadata": {
        "id": "lKILSyDpKlXh"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l6C_-E1fLcWG"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZxGZ7AXGLDDS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    'gpt2',\n",
        "    torch_dtype=torch.float64,\n",
        ")\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJSo4Jx6gdHs",
        "outputId": "877d0959-d7d0-4c9b-c0f0-6571553ca790"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr= #Your value)\n",
        "N_ITERATIONS = 100\n",
        "cur_iteration = 0\n",
        "loss_values = []\n",
        "for x in train_batch_loader:\n",
        "    if cur_iteration == N_ITERATIONS:\n",
        "      break\n",
        "\n",
        "    model.train()\n",
        "    #################\n",
        "    input_tokens = x['input_ids'].to(device)\n",
        "    attention_mask = x['input_att_mask'].to(device)\n",
        "    labels = x['labels'].clone().to(device)\n",
        "    out_logits = model(input_ids=input_tokens, attention_mask=attention_mask).logits\n",
        "\n",
        "    ### не затираем токены начала и конца, чтобы модель училась останавливаться ###\n",
        "    labels[labels == pad_token_id] = -100\n",
        "\n",
        "    loss_value = loss(out_logits.permute(0, 2, 1), labels)\n",
        "    #################\n",
        "    print(f\"Loss value: {loss_value.item()}\")\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    cur_iteration += 1\n",
        "    loss_values.append(loss_value.cpu().detach().numpy())"
      ],
      "metadata": {
        "id": "rxRAl4lygNuB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data for plotting\n",
        "t = [i for i in range(len(loss_values))]\n",
        "s = loss_values\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(t, s)\n",
        "\n",
        "ax.set(xlabel='step', ylabel='loss value',\n",
        "       title='Train Loss')\n",
        "ax.grid()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OnE3oz-FgNwj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n79BGpGMgNzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VOxYHnw825Rf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}