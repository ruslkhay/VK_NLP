{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1\n",
    "\n",
    "Ниже даны матрица $X$ (каждая строка - эмбеддинг очередного токена) и матрицы проекций $W_Q$, $W_K$ и $W_V$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[-1.51, -1.57,  0.87,  1.1 , -0.74],\n",
    "                  [ 0.46, -0.4 , -0.64,  0.12, -0.02],\n",
    "                  [-0.75,  0.44,  1.05,  0.38, -0.16]])\n",
    "W_Q = torch.tensor([[ 1.88, -0.04, -1.51,  0.5 ,  0.18],\n",
    "                    [-0.79,  1.06,  0.09, -0.3 ,  0.55],\n",
    "                    [ 0.03,  0.44,  3.06, -1.91,  1.52],\n",
    "                    [-0.1 , -2.17,  0.93,  0.82, -0.35],\n",
    "                    [ 0.27,  0.54, -0.42, -0.8 ,  1.41]])\n",
    "W_K = torch.tensor([[-0.03,  1.33, -1.91, -1.73,  0.73],\n",
    "                    [ 1.06,  0.08,  1.01,  0.9 , -0.  ],\n",
    "                    [ 0.17, -0.11, -0.11, -0.49,  0.7 ],\n",
    "                    [-0.66, -1.44, -0.56,  0.95, -0.72],\n",
    "                    [-0.5 , -1.2 ,  1.59, -0.47, -0.34]])\n",
    "W_V = torch.tensor([[-1.55, -1.48,  2.23,  0.57, -1.53],\n",
    "                    [-1.45, -0.91, -1.69,  0.43,  0.44],\n",
    "                    [-1.05,  0.19, -0.65, -0.34,  0.12],\n",
    "                    [-1.29,  1.48,  0.18,  0.24,  0.83],\n",
    "                    [ 2.12,  1.09,  0.79, -0.21, -0.95]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос:** Каким будет новое представление входной последовательности после применения self-attention?\n",
    "\n",
    "Полезные операции со ссылками на документацию: [`torch.matmul`](https://pytorch.org/docs/stable/generated/torch.matmul.html), [`F.softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html), [`math.sqrt`](https://docs.python.org/3/library/math.html) и метод [`transpose`](https://pytorch.org/docs/stable/generated/torch.Tensor.transpose.html) у тензора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3952,  2.8954, -2.4458, -0.9194,  2.6043],\n",
      "        [ 0.5535,  4.1379, -1.5410, -1.2466,  3.0007],\n",
      "        [-1.2202,  1.3002, -2.7521, -0.4466,  1.7936]])\n"
     ]
    }
   ],
   "source": [
    "Q = torch.matmul(X, W_Q)\n",
    "K = torch.matmul(X, W_K)\n",
    "V = torch.matmul(X, W_V)\n",
    "a = torch.matmul(Q, K.transpose(dim0=1, dim1=0)) / math.sqrt(X.shape[1])\n",
    "alpha = F.softmax(a, dim=1)\n",
    "z = torch.matmul(alpha, V)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2\n",
    "\n",
    "В этом задании и далее мы будем работать с библиотекой [`transformers`](https://huggingface.co/docs/transformers/index) от [`HuggingFace`](https://huggingface.co/). На сегодняшний день это, пожалуй, самая популярная и удобная библиотека для работы с моделями на базе трансформерной архитектуры.\n",
    "\n",
    "В качестве [`модели`](https://huggingface.co/utrobinmv/t5_translate_en_ru_zh_small_1024) возьмем [`T5`](https://huggingface.co/docs/transformers/model_doc/t5), обученную на задачу перевода с одного языка на другой.\n",
    "\n",
    "Прежде всего, загрузим токенизатор, который будет превращать текст в последовательность номеров токенов из словаря."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/oplora/Documents/VMK/VK_NLP/nlp/lib/python3.10/site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597cdc5e84134efe9ae9cc6caccfdec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/1.47M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e09d142b0b4f66af851d10e29a6d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda42336f0b5474a9bfbe5a5649ed7fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('utrobinmv/t5_translate_en_ru_zh_small_1024', use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для примера попробуем превратить строку \"Мама мыла раму\" в последовательность номеров токенов.\n",
    "Для этого нужно вызвать [`tokenizer(text)`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__), где text - текст, который мы хотим токенизировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [43690, 290, 391, 4, 6587, 57, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer('Мама мыла раму')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что токенизатор превратил текст в последовательность номеров токенов из словаря (поле input_ids), а также расчитал маску внимния (поле attention_mask).\n",
    "\n",
    "Последовательность номеров токенов можно декодировать обратно в текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мама мыла раму</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokens['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T13:17:08.597483Z",
     "iopub.status.busy": "2024-03-03T13:17:08.596947Z",
     "iopub.status.idle": "2024-03-03T13:17:08.606504Z",
     "shell.execute_reply": "2024-03-03T13:17:08.604716Z",
     "shell.execute_reply.started": "2024-03-03T13:17:08.597442Z"
    }
   },
   "source": [
    "Видно, что токенизатор автоматически добавил `</s>` - токен конца входной последовательности. Попробуем декодировать каждый номер (token-id) отдельно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43690 Мама\n",
      "290 мы\n",
      "391 ла\n",
      "4 \n",
      "6587 рам\n",
      "57 у\n",
      "1 </s>\n"
     ]
    }
   ],
   "source": [
    "for token_id in tokens['input_ids']:\n",
    "    print(token_id, tokenizer.decode(token_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что мы имеем дело с [`subword-токенизацией`](https://huggingface.co/docs/transformers/tokenizer_summary#subword-tokenization) - некоторые слова представляются одним токеном, но некоторые разбиваются на несколько.\n",
    "\n",
    "Можно заглянуть в словарь и напрямую:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <pad>\n",
      "1 </s>\n",
      "2 <unk>\n",
      "3 ,\n",
      "4 ▁\n",
      "5 .\n",
      "6 0\n",
      "7 ▁the\n",
      "8 1\n",
      "9 -\n",
      "10 2\n",
      "11 ▁of\n",
      "12 ▁and\n",
      "13 s\n",
      "14 ▁в\n",
      "15 9\n",
      "16 ▁и\n",
      "17 3\n",
      "18 5\n",
      "19 ▁to\n",
      "20 4\n",
      "21 ▁in\n",
      "22 6\n",
      "23 /\n",
      "24 8\n",
      "25 ▁(\n",
      "26 7\n",
      "27 ▁a\n",
      "28 )\n",
      "29 的\n",
      "30 。\n",
      "31 :\n",
      "32 а\n",
      "33 ▁на\n",
      "34 ▁с\n",
      "35 ▁-\n",
      "36 ▁is\n",
      "37 ▁for\n",
      "38 '\n",
      "39 ▁\"\n",
      "40 和\n",
      "41 ▁по\n",
      "42 ▁that\n",
      "43 、\n",
      "44 е\n",
      "45 ▁on\n",
      "46 ▁with\n",
      "47 (\n",
      "48 и\n",
      "49 ▁The\n",
      "50 ;\n",
      "51 ▁не\n",
      "52 ▁что\n",
      "53 ▁as\n",
      "54 ▁by\n",
      "55 ы\n",
      "56 м\n",
      "57 у\n",
      "58 ▁was\n",
      "59 在\n",
      "60 ▁from\n",
      "61 ing\n",
      "62 ▁для\n",
      "63 ▁be\n",
      "64 ▁are\n",
      "65 ▁В\n",
      "66 \"\n",
      "67 ▁or\n",
      "68 ▁at\n",
      "69 о\n",
      "70 ом\n",
      "71 ed\n",
      "72 ▁из\n",
      "73 ▁it\n",
      "74 ),\n",
      "75 ▁к\n",
      "76 年\n",
      "77 ▁от\n",
      "78 ▁о\n",
      "79 ▁за\n",
      "80 ▁«\n",
      "81 ▁как\n",
      "82 ов\n",
      "83 ▁an\n",
      "84 ).\n",
      "85 ▁I\n",
      "86 A\n",
      "87 ▁you\n",
      "88 d\n",
      "89 ▁which\n",
      "90 ▁not\n",
      "91 ▁have\n",
      "92 ▁или\n",
      "93 t\n",
      "94 х\n",
      "95 ▁A\n",
      "96 ▁this\n",
      "97 ▁In\n",
      "98 a\n",
      "99 ?\n"
     ]
    }
   ],
   "source": [
    "token_to_id = tokenizer.get_vocab()\n",
    "id_to_token = {token_id: token for token, token_id in token_to_id.items()}\n",
    "\n",
    "for i in range(100):\n",
    "    print(i, id_to_token[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T13:28:04.850368Z",
     "iopub.status.busy": "2024-03-03T13:28:04.849473Z",
     "iopub.status.idle": "2024-03-03T13:28:04.861229Z",
     "shell.execute_reply": "2024-03-03T13:28:04.859469Z",
     "shell.execute_reply.started": "2024-03-03T13:28:04.850314Z"
    }
   },
   "source": [
    "Видно, что часть токенов начинается с нижнего подчеркивания. Это означает, что данный токен является началом какого-то слова и не может быть другой частью слова (например, стоять в середине или в конце). Посмотрим, например, как токенизируются строки \"I and you\" и \"Iandyou\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [85, 12, 87, 1], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'input_ids': [85, 370, 4760, 1], 'attention_mask': [1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"I and you\"))\n",
    "print(tokenizer(\"Iandyou\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁and\n",
      "and\n"
     ]
    }
   ],
   "source": [
    "print(id_to_token[12])\n",
    "print(id_to_token[370])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T13:39:40.024659Z",
     "iopub.status.busy": "2024-03-03T13:39:40.024112Z",
     "iopub.status.idle": "2024-03-03T13:39:40.033414Z",
     "shell.execute_reply": "2024-03-03T13:39:40.03168Z",
     "shell.execute_reply.started": "2024-03-03T13:39:40.024618Z"
    }
   },
   "source": [
    "Видно, что токен \"and\" имеет две записи в словере (для начала слова и для других расположений). Но при этом при декодировании токенизатор убирает нижнее подчеркивание:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\n",
      "and\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(12))\n",
    "print(tokenizer.decode(370))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T13:42:33.329603Z",
     "iopub.status.busy": "2024-03-03T13:42:33.329126Z",
     "iopub.status.idle": "2024-03-03T13:42:33.338662Z",
     "shell.execute_reply": "2024-03-03T13:42:33.33676Z",
     "shell.execute_reply.started": "2024-03-03T13:42:33.329553Z"
    }
   },
   "source": [
    "При токенизации можно попросить токенизатор не добавлять токен конца последовательности. Для этого нужно выставить `add_special_tokens=False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [43690, 290, 391, 4, 6587, 57], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Мама мыла раму', add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T13:43:45.318462Z",
     "iopub.status.busy": "2024-03-03T13:43:45.317875Z",
     "iopub.status.idle": "2024-03-03T13:43:45.326437Z",
     "shell.execute_reply": "2024-03-03T13:43:45.324882Z",
     "shell.execute_reply.started": "2024-03-03T13:43:45.318419Z"
    }
   },
   "source": [
    "**Вопрос:** Какой номер в словаре имеет токен \"▁наука\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24880\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "print(vocab['▁наука'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T13:55:07.079884Z",
     "iopub.status.busy": "2024-03-03T13:55:07.078258Z",
     "iopub.status.idle": "2024-03-03T13:55:07.085716Z",
     "shell.execute_reply": "2024-03-03T13:55:07.084546Z",
     "shell.execute_reply.started": "2024-03-03T13:55:07.079824Z"
    }
   },
   "source": [
    "### Задание 3\n",
    "\n",
    "Токенизируйте строку \"LLM - это смысл моей жизни\" с `add_special_tokens=False`. В какую последовательность номеров токенизатор перевел строку? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [503, 16322, 35, 140, 12440, 8949, 737], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer('LLM - это смысл моей жизни', add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T14:07:12.133237Z",
     "iopub.status.busy": "2024-03-03T14:07:12.132705Z",
     "iopub.status.idle": "2024-03-03T14:07:12.140114Z",
     "shell.execute_reply": "2024-03-03T14:07:12.138544Z",
     "shell.execute_reply.started": "2024-03-03T14:07:12.133203Z"
    }
   },
   "source": [
    "### Задание 4\n",
    "\n",
    "Теперь загрузим модель и сгенерируем перевод для предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e15c02d1c94817ac37ff8b849142a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/809 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cae7d9434834e2d8b962e0575295031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b447cb18eeee4f89b0661cd0f577622e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(65100, 512)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(65100, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 6)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-7): 7 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(65100, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 6)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-7): 7 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=65100, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('utrobinmv/t5_translate_en_ru_zh_small_1024')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что модель состоит из кодировщика и декодировщика. Посмотрим, сколько параметров она имеет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110724480\n"
     ]
    }
   ],
   "source": [
    "print(model.num_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переведем строку \"Без труда не выловишь и рыбку из пруда.\" с русского языка на английский. Для этого нужно особым образом сконструировать запрос ([`примеры`](https://huggingface.co/utrobinmv/t5_translate_en_ru_zh_small_1024)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = 'translate to en: Без труда не выловишь и рыбку из пруда.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T14:23:44.29665Z",
     "iopub.status.busy": "2024-03-03T14:23:44.296087Z",
     "iopub.status.idle": "2024-03-03T14:23:44.304116Z",
     "shell.execute_reply": "2024-03-03T14:23:44.302371Z",
     "shell.execute_reply.started": "2024-03-03T14:23:44.296613Z"
    }
   },
   "source": [
    "Превратим текст в номера токенов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[21809,    19,  1904,    31, 10401,  3306,    51,   160,  2406,  9466,\n",
      "            16,  6596,   435,    72, 46769,    32,     5,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "x = tokenizer(src_text, return_tensors='pt')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T14:29:05.287876Z",
     "iopub.status.busy": "2024-03-03T14:29:05.287338Z",
     "iopub.status.idle": "2024-03-03T14:29:05.296886Z",
     "shell.execute_reply": "2024-03-03T14:29:05.294694Z",
     "shell.execute_reply.started": "2024-03-03T14:29:05.287837Z"
    }
   },
   "source": [
    "Генерируем выходную последовательность:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0, 20040,   327,     3,  1182,    38,    93, 15373,     7,  3958,\n",
      "           217,    11,     7,     4, 18263,     5,     1]])\n"
     ]
    }
   ],
   "source": [
    "output_ids = model.generate(x['input_ids'])\n",
    "print(output_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T14:30:14.729808Z",
     "iopub.status.busy": "2024-03-03T14:30:14.729303Z",
     "iopub.status.idle": "2024-03-03T14:30:14.737859Z",
     "shell.execute_reply": "2024-03-03T14:30:14.736103Z",
     "shell.execute_reply.started": "2024-03-03T14:30:14.729772Z"
    }
   },
   "source": [
    "Превращаем последовательность номеров токенов в текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without work, don't catch the fish out of the pond.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос:** Какой перевод модель сгенерирует для текста \"Однажды, в студеную зимнюю пору. Я из лесу вышел; был сильный мороз. Гляжу, поднимается медленно в гору. Лошадка, везущая хворосту воз.\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One day, I went out of the woods; there was a strong frost. I'm going to climb slowly into the mountain. A horse that is lucky for you.\n"
     ]
    }
   ],
   "source": [
    "src_text = 'translate to en: Однажды, в студеную зимнюю пору. Я из лесу вышел; был сильный мороз. Гляжу, поднимается медленно в гору. Лошадка, везущая хворосту воз.'\n",
    "x = tokenizer(src_text, return_tensors='pt')\n",
    "output_ids = model.generate(x['input_ids'])\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5\n",
    "\n",
    "Попробуем обучить модель генерировать ответы на следующие вопросы:\n",
    "1. Какой город является столицей России?\n",
    "2. Какой город является столицей Соединенных Штатов Америки?\n",
    "\n",
    "Как обычно, для пары (вопрос, ответ) получить loss и минимизировать его. Для примера получим loss для для пары (\"Какой город является столицей Финляндии?\", \"Хельсинки\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = \"Какой город является столицей Финляндии?\"\n",
    "tgt_text = \"Хельсинки\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Превратим входной и выходной тексты в последовательности номеров токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  743,   108,  2143,   277, 37909, 15859,    99,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[37136,  5749,   182,     1]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "src_ids = tokenizer(src_text, return_tensors='pt')\n",
    "tgt_ids = tokenizer(tgt_text, return_tensors='pt')\n",
    "\n",
    "print(src_ids)\n",
    "print(tgt_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим модель, передав ей входную и выходную последовательность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['loss', 'logits', 'past_key_values', 'encoder_last_hidden_state'])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(input_ids=src_ids['input_ids'], labels=tgt_ids['input_ids'])\n",
    "print(output.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что модель вернула loss для выходной последовательности, а также логиты для данного токена (чтобы мы могли посчитать какой-нибудь свой loss, если захотим). Посмотрим, чему равен loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.3770)\n"
     ]
    }
   ],
   "source": [
    "print(output.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лосс получился очень большой. Посмотрим, какой ответ модель сейчас генерирует на наш вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What kind of city is the capital of Finland?\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model.generate(src_ids['input_ids'])[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель сгенерировала перевод вопроса, что неудивительно, так как ее учили только переводить тексты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос**: Какой loss у пары (\"translate to en: Вчера мы ходили в зоопарк и видели там смешных капибар.\", \"Yesterday we went to the zoo and saw funny capybaras there.\")?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4681)\n",
      "Ellipsis\n"
     ]
    }
   ],
   "source": [
    "src_text = \"translate to en: Вчера мы ходили в зоопарк и видели там смешных капибар.\"\n",
    "tgt_text = \"Yesterday we went to the zoo and saw funny capybaras there.\"\n",
    "    \n",
    "src_ids = tokenizer(src_text, return_tensors='pt')\n",
    "tgt_ids = tokenizer(tgt_text, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids=src_ids['input_ids'], labels=tgt_ids['input_ids'])\n",
    "print(output.loss)\n",
    "print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приступим, наконец, к обучению. Подготовим данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\"Какой город является столицей России?\", \"Какой город является столицей Новой Зеландии?\"]\n",
    "output_texts = [\"Москва\", \"Веллингтон\"]\n",
    "\n",
    "train_data = []\n",
    "for input_text, output_text in zip(input_texts, output_texts):\n",
    "    x = tokenizer(input_text)\n",
    "    y = tokenizer(output_text)\n",
    "    x['labels'] = y['input_ids']\n",
    "    train_data.append(x)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим data collator для выравнивания последовательностей по максимальной длине с использованием pad-токенов и их маскировки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что он делает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_collator(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что\n",
    "* Массив словарей превратился в \"словарь массивов\": в каждом поле содержится батч.\n",
    "* input_ids дополнились до максимальной длины нулями (все последовательности теперь имеют одну и ту же длину).\n",
    "* attention_mask тоже дополнился нулями (не будем учитывать pad-токены)\n",
    "* labels дополнился -100 (не будем считать loss по pad-токенам)\n",
    "\n",
    "Запускаем обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = Seq2SeqTrainingArguments(\n",
    "    './output',\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    max_steps=100,\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(); # Переводим модель в режим обучения (включаем Dropout, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(model, train_args, train_dataset=train_data, data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем, научилась ли наша модель отвечать на два вопроса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval(); # Переводим модель в режим инференса (Отключаем Dropout, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_ids = tokenizer('\"Какой город является столицей России?\"', return_tensors='pt')\n",
    "print(tokenizer.decode(model.generate(src_ids['input_ids'])[0], skip_special_tokens=True))\n",
    "\n",
    "src_ids = tokenizer('\"Какой город является столицей Новой Зеландии?\"', return_tensors='pt')\n",
    "print(tokenizer.decode(model.generate(src_ids['input_ids'])[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
