{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1\n",
    "\n",
    "Ниже даны матрица $X$ (каждая строка - эмбеддинг очередного токена) и матрицы проекций $W_Q$, $W_K$ и $W_V$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[-1.51, -1.57,  0.87,  1.1 , -0.74],\n",
    "                  [ 0.46, -0.4 , -0.64,  0.12, -0.02],\n",
    "                  [-0.75,  0.44,  1.05,  0.38, -0.16]])\n",
    "W_Q = torch.tensor([[ 1.88, -0.04, -1.51,  0.5 ,  0.18],\n",
    "                    [-0.79,  1.06,  0.09, -0.3 ,  0.55],\n",
    "                    [ 0.03,  0.44,  3.06, -1.91,  1.52],\n",
    "                    [-0.1 , -2.17,  0.93,  0.82, -0.35],\n",
    "                    [ 0.27,  0.54, -0.42, -0.8 ,  1.41]])\n",
    "W_K = torch.tensor([[-0.03,  1.33, -1.91, -1.73,  0.73],\n",
    "                    [ 1.06,  0.08,  1.01,  0.9 , -0.  ],\n",
    "                    [ 0.17, -0.11, -0.11, -0.49,  0.7 ],\n",
    "                    [-0.66, -1.44, -0.56,  0.95, -0.72],\n",
    "                    [-0.5 , -1.2 ,  1.59, -0.47, -0.34]])\n",
    "W_V = torch.tensor([[-1.55, -1.48,  2.23,  0.57, -1.53],\n",
    "                    [-1.45, -0.91, -1.69,  0.43,  0.44],\n",
    "                    [-1.05,  0.19, -0.65, -0.34,  0.12],\n",
    "                    [-1.29,  1.48,  0.18,  0.24,  0.83],\n",
    "                    [ 2.12,  1.09,  0.79, -0.21, -0.95]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос:** Каким будет новое представление входной последовательности после применения self-attention?\n",
    "\n",
    "Полезные операции со ссылками на документацию: [`torch.matmul`](https://pytorch.org/docs/stable/generated/torch.matmul.html), [`F.softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html), [`math.sqrt`](https://docs.python.org/3/library/math.html) и метод [`transpose`](https://pytorch.org/docs/stable/generated/torch.Tensor.transpose.html) у тензора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = ...\n",
    "K = ...\n",
    "V = ...\n",
    "a = ...\n",
    "alpha = ...\n",
    "z = ...\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2\n",
    "\n",
    "В этом задании и далее мы будем работать с библиотекой [`transformers`](https://huggingface.co/docs/transformers/index) от [`HuggingFace`](https://huggingface.co/). На сегодняшний день это, пожалуй, самая популярная и удобная библиотека для работы с моделями на базе трансформерной архитектуры.\n",
    "\n",
    "В качестве [`модели`](https://huggingface.co/utrobinmv/t5_translate_en_ru_zh_small_1024) возьмем [`T5`](https://huggingface.co/docs/transformers/model_doc/t5), обученную на задачу перевода с одного языка на другой.\n",
    "\n",
    "Прежде всего, загрузим токенизатор, который будет превращать текст в последовательность номеров токенов из словаря."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('utrobinmv/t5_translate_en_ru_zh_small_1024', use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для примера попробуем превратить строку \"Мама мыла раму\" в последовательность номеров токенов.\n",
    "Для этого нужно вызвать [`tokenizer(text)`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__), где text - текст, который мы хотим токенизировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer('Мама мыла раму')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что токенизатор превратил текст в последовательность номеров токенов из словаря (поле input_ids), а также расчитал маску внимния (поле attention_mask).\n",
    "\n",
    "Последовательность номеров токенов можно декодировать обратно в текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(tokens['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T13:17:08.597483Z",
     "iopub.status.busy": "2024-03-03T13:17:08.596947Z",
     "iopub.status.idle": "2024-03-03T13:17:08.606504Z",
     "shell.execute_reply": "2024-03-03T13:17:08.604716Z",
     "shell.execute_reply.started": "2024-03-03T13:17:08.597442Z"
    }
   },
   "source": [
    "Видно, что токенизатор автоматически добавил `</s>` - токен конца входной последовательности. Попробуем декодировать каждый номер (token-id) отдельно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token_id in tokens['input_ids']:\n",
    "    print(token_id, tokenizer.decode(token_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что мы имеем дело с [`subword-токенизацией`](https://huggingface.co/docs/transformers/tokenizer_summary#subword-tokenization) - некоторые слова представляются одним токеном, но некоторые разбиваются на несколько.\n",
    "\n",
    "Можно заглянуть в словарь и напрямую:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = tokenizer.get_vocab()\n",
    "id_to_token = {token_id: token for token, token_id in token_to_id.items()}\n",
    "\n",
    "for i in range(100):\n",
    "    print(i, id_to_token[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T13:28:04.850368Z",
     "iopub.status.busy": "2024-03-03T13:28:04.849473Z",
     "iopub.status.idle": "2024-03-03T13:28:04.861229Z",
     "shell.execute_reply": "2024-03-03T13:28:04.859469Z",
     "shell.execute_reply.started": "2024-03-03T13:28:04.850314Z"
    }
   },
   "source": [
    "Видно, что часть токенов начинается с нижнего подчеркивания. Это означает, что данный токен является началом какого-то слова и не может быть другой частью слова (например, стоять в середине или в конце). Посмотрим, например, как токенизируются строки \"I and you\" и \"Iandyou\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer(\"I and you\"))\n",
    "print(tokenizer(\"Iandyou\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id_to_token[12])\n",
    "print(id_to_token[370])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T13:39:40.024659Z",
     "iopub.status.busy": "2024-03-03T13:39:40.024112Z",
     "iopub.status.idle": "2024-03-03T13:39:40.033414Z",
     "shell.execute_reply": "2024-03-03T13:39:40.03168Z",
     "shell.execute_reply.started": "2024-03-03T13:39:40.024618Z"
    }
   },
   "source": [
    "Видно, что токен \"and\" имеет две записи в словере (для начала слова и для других расположений). Но при этом при декодировании токенизатор убирает нижнее подчеркивание:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(12))\n",
    "print(tokenizer.decode(370))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T13:42:33.329603Z",
     "iopub.status.busy": "2024-03-03T13:42:33.329126Z",
     "iopub.status.idle": "2024-03-03T13:42:33.338662Z",
     "shell.execute_reply": "2024-03-03T13:42:33.33676Z",
     "shell.execute_reply.started": "2024-03-03T13:42:33.329553Z"
    }
   },
   "source": [
    "При токенизации можно попросить токенизатор не добавлять токен конца последовательности. Для этого нужно выставить `add_special_tokens=False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer('Мама мыла раму', add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T13:43:45.318462Z",
     "iopub.status.busy": "2024-03-03T13:43:45.317875Z",
     "iopub.status.idle": "2024-03-03T13:43:45.326437Z",
     "shell.execute_reply": "2024-03-03T13:43:45.324882Z",
     "shell.execute_reply.started": "2024-03-03T13:43:45.318419Z"
    }
   },
   "source": [
    "**Вопрос:** Какой номер в словаре имеет токен \"▁наука\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T13:55:07.079884Z",
     "iopub.status.busy": "2024-03-03T13:55:07.078258Z",
     "iopub.status.idle": "2024-03-03T13:55:07.085716Z",
     "shell.execute_reply": "2024-03-03T13:55:07.084546Z",
     "shell.execute_reply.started": "2024-03-03T13:55:07.079824Z"
    }
   },
   "source": [
    "### Задание 3\n",
    "\n",
    "Токенизируйте строку \"LLM - это смысл моей жизни\" с `add_special_tokens=False`. В какую последовательность номеров токенизатор перевел строку? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T14:07:12.133237Z",
     "iopub.status.busy": "2024-03-03T14:07:12.132705Z",
     "iopub.status.idle": "2024-03-03T14:07:12.140114Z",
     "shell.execute_reply": "2024-03-03T14:07:12.138544Z",
     "shell.execute_reply.started": "2024-03-03T14:07:12.133203Z"
    }
   },
   "source": [
    "### Задание 4\n",
    "\n",
    "Теперь загрузим модель и сгенерируем перевод для предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('utrobinmv/t5_translate_en_ru_zh_small_1024')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что модель состоит из кодировщика и декодировщика. Посмотрим, сколько параметров она имеет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.num_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переведем строку \"Без труда не выловишь и рыбку из пруда.\" с русского языка на английский. Для этого нужно особым образом сконструировать запрос ([`примеры`](https://huggingface.co/utrobinmv/t5_translate_en_ru_zh_small_1024)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = 'translate to en: Без труда не выловишь и рыбку из пруда.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T14:23:44.29665Z",
     "iopub.status.busy": "2024-03-03T14:23:44.296087Z",
     "iopub.status.idle": "2024-03-03T14:23:44.304116Z",
     "shell.execute_reply": "2024-03-03T14:23:44.302371Z",
     "shell.execute_reply.started": "2024-03-03T14:23:44.296613Z"
    }
   },
   "source": [
    "Превратим текст в номера токенов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer(src_text, return_tensors='pt')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T14:29:05.287876Z",
     "iopub.status.busy": "2024-03-03T14:29:05.287338Z",
     "iopub.status.idle": "2024-03-03T14:29:05.296886Z",
     "shell.execute_reply": "2024-03-03T14:29:05.294694Z",
     "shell.execute_reply.started": "2024-03-03T14:29:05.287837Z"
    }
   },
   "source": [
    "Генерируем выходную последовательность:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = model.generate(x['input_ids'])\n",
    "print(output_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T14:30:14.729808Z",
     "iopub.status.busy": "2024-03-03T14:30:14.729303Z",
     "iopub.status.idle": "2024-03-03T14:30:14.737859Z",
     "shell.execute_reply": "2024-03-03T14:30:14.736103Z",
     "shell.execute_reply.started": "2024-03-03T14:30:14.729772Z"
    }
   },
   "source": [
    "Превращаем последовательность номеров токенов в текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос:** Какой перевод модель сгенерирует для текста \"Однажды, в студеную зимнюю пору. Я из лесу вышел; был сильный мороз. Гляжу, поднимается медленно в гору. Лошадка, везущая хворосту воз.\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = 'translate to en: Однажды, в студеную зимнюю пору. Я из лесу вышел; был сильный мороз. Гляжу, поднимается медленно в гору. Лошадка, везущая хворосту воз.'\n",
    "print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5\n",
    "\n",
    "Попробуем обучить модель генерировать ответы на следующие вопросы:\n",
    "1. Какой город является столицей России?\n",
    "2. Какой город является столицей Соединенных Штатов Америки?\n",
    "\n",
    "Как обычно, для пары (вопрос, ответ) получить loss и минимизировать его. Для примера получим loss для для пары (\"Какой город является столицей Финляндии?\", \"Хельсинки\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = \"Какой город является столицей Финляндии?\"\n",
    "tgt_text = \"Хельсинки\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Превратим входной и выходной тексты в последовательности номеров токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_ids = tokenizer(src_text, return_tensors='pt')\n",
    "tgt_ids = tokenizer(tgt_text, return_tensors='pt')\n",
    "\n",
    "print(src_ids)\n",
    "print(tgt_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим модель, передав ей входную и выходную последовательность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(input_ids=src_ids['input_ids'], labels=tgt_ids['input_ids'])\n",
    "print(output.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что модель вернула loss для выходной последовательности, а также логиты для данного токена (чтобы мы могли посчитать какой-нибудь свой loss, если захотим). Посмотрим, чему равен loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лосс получился очень большой. Посмотрим, какой ответ модель сейчас генерирует на наш вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(model.generate(src_ids['input_ids'])[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель сгенерировала перевод вопроса, что неудивительно, так как ее учили только переводить тексты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос**: Какой loss у пары (\"translate to en: Вчера мы ходили в зоопарк и видели там смешных капибар.\", \"Yesterday we went to the zoo and saw funny capybaras there.\")?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = \"translate to en: Вчера мы ходили в зоопарк и видели там смешных капибар.\"\n",
    "tgt_text = \"Yesterday we went to the zoo and saw funny capybaras there.\"\n",
    "    \n",
    "print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приступим, наконец, к обучению. Подготовим данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\"Какой город является столицей России?\", \"Какой город является столицей Новой Зеландии?\"]\n",
    "output_texts = [\"Москва\", \"Веллингтон\"]\n",
    "\n",
    "train_data = []\n",
    "for input_text, output_text in zip(input_texts, output_texts):\n",
    "    x = tokenizer(input_text)\n",
    "    y = tokenizer(output_text)\n",
    "    x['labels'] = y['input_ids']\n",
    "    train_data.append(x)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим data collator для выравнивания последовательностей по максимальной длине с использованием pad-токенов и их маскировки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что он делает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_collator(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что\n",
    "* Массив словарей превратился в \"словарь массивов\": в каждом поле содержится батч.\n",
    "* input_ids дополнились до максимальной длины нулями (все последовательности теперь имеют одну и ту же длину).\n",
    "* attention_mask тоже дополнился нулями (не будем учитывать pad-токены)\n",
    "* labels дополнился -100 (не будем считать loss по pad-токенам)\n",
    "\n",
    "Запускаем обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = Seq2SeqTrainingArguments(\n",
    "    './output',\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    max_steps=100,\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(); # Переводим модель в режим обучения (включаем Dropout, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(model, train_args, train_dataset=train_data, data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем, научилась ли наша модель отвечать на два вопроса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval(); # Переводим модель в режим инференса (Отключаем Dropout, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_ids = tokenizer('\"Какой город является столицей России?\"', return_tensors='pt')\n",
    "print(tokenizer.decode(model.generate(src_ids['input_ids'])[0], skip_special_tokens=True))\n",
    "\n",
    "src_ids = tokenizer('\"Какой город является столицей Новой Зеландии?\"', return_tensors='pt')\n",
    "print(tokenizer.decode(model.generate(src_ids['input_ids'])[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
