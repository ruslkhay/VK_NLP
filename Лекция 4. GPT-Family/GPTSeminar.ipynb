{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install datasets==2.16.1"
      ],
      "metadata": {
        "id": "fwrjccfsem1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDEtbNmJNP5i"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "from datasets import load_dataset, list_datasets, Dataset\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2Model\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/model_doc/gpt2"
      ],
      "metadata": {
        "id": "N8Hla8m3wgjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/main_classes/tokenizer"
      ],
      "metadata": {
        "id": "oJ51auF9rop7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "G9nI6GwqNlSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# DEVICE = torch.device(\"cpu\")\n",
        "print(f\"Our device is {DEVICE}\")"
      ],
      "metadata": {
        "id": "Ddvb-7swo6t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Как работает генерация с т.з. кода под капотом:"
      ],
      "metadata": {
        "id": "jstVDi-2Y-7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT_INPUT = \"Парламент- это не место для\""
      ],
      "metadata": {
        "id": "frkwEHCUVhIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(TEXT_INPUT, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "zDqkoPR9VfPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in inputs.items():\n",
        "  inputs[k] = v.to(DEVICE)"
      ],
      "metadata": {
        "id": "ASX286Q2rb4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bare_model = GPT2Model.from_pretrained(\"gpt2\")\n",
        "bare_model.eval()\n",
        "bare_model.to(DEVICE)\n",
        "bare_outputs = bare_model(**inputs, output_hidden_states=True)"
      ],
      "metadata": {
        "id": "OEAJfqmyiqWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bare_model"
      ],
      "metadata": {
        "id": "slyM9NGEZfFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_hidden_states = bare_outputs.last_hidden_state"
      ],
      "metadata": {
        "id": "HKT71AzAvbJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weight tying"
      ],
      "metadata": {
        "id": "lBExneKSwz_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Image(url= \"https://lena-voita.github.io/resources/lectures/lang_models/practical/weight_tying_idea-min.png\", width=1900, height=900)"
      ],
      "metadata": {
        "id": "IQl0LxZjwqfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = torch.matmul(\n",
        "    last_hidden_states[-1][-1],\n",
        "    bare_model.wte.weight.T\n",
        ")"
      ],
      "metadata": {
        "id": "mezRRFrkjzTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bare_probas = F.softmax(logits, dim=0)"
      ],
      "metadata": {
        "id": "C6tdRDgsj8W-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.argmax(bare_probas)"
      ],
      "metadata": {
        "id": "wZLo69g5j9VC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Упрощённая генерация:"
      ],
      "metadata": {
        "id": "xZSih2orj-ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
        "llm_model.eval()\n",
        "llm_model.to(DEVICE)\n",
        "llm_outputs = llm_model(**inputs)"
      ],
      "metadata": {
        "id": "tBcgTY12kV5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_params = 0\n",
        "for param in llm_model.parameters(recurse=True):\n",
        "    n_params += param.numel()\n",
        "\n",
        "\n",
        "n_params = str(n_params)\n",
        "n_params = \",\".join(\n",
        "    [\n",
        "        n_params[i: i+3]\n",
        "        for i in range(\n",
        "            len(n_params) - 3, -1, -3\n",
        "        )\n",
        "    ][::-1]\n",
        ")\n",
        "print(f\"Number of parameters: {n_params}\")"
      ],
      "metadata": {
        "id": "-Uoilh3vnxMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.argmax(llm_outputs.logits[-1][-1])"
      ],
      "metadata": {
        "id": "DiWJ1qWXkk7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_probas = F.softmax(llm_outputs.logits[-1][-1], dim=0)"
      ],
      "metadata": {
        "id": "0GcBrRvrtBRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.allclose(bare_probas, llm_probas, rtol=1e-4)"
      ],
      "metadata": {
        "id": "Uw1dNB-RtVW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 1: написать свою имплементацию BPE"
      ],
      "metadata": {
        "id": "CmhH8-dPZG9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как работает алгоритм: \\\n",
        "- У алгоритма один гиперпараметр- число итераций \\\n",
        "- На каждой итерации мы находим самую популярную пару токенов, идущих подряд \\(для примера наховём их a, b) \\\n",
        "- Мы создаём новый токен, соответствующий конактенции пары из предыдущего пункта \\(a,b -> ab), все вхождения пары в тренировочных данных заменяем на новый токен"
      ],
      "metadata": {
        "id": "iOVbyjS0ZUUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_training_data(training_data, max_key):\n",
        "    n_tokens = len(training_data)\n",
        "    training_data_tmp = []\n",
        "    i = 0\n",
        "    while i < n_tokens - 1:\n",
        "        if training_data[i] + training_data[i+1] == max_key:\n",
        "            training_data_tmp.append(max_key)\n",
        "            i += 2\n",
        "        else:\n",
        "            training_data_tmp.append(training_data[i])\n",
        "            i += 1\n",
        "\n",
        "    if i == n_tokens - 1:\n",
        "        training_data_tmp.append(training_data[i])\n",
        "\n",
        "    return training_data_tmp"
      ],
      "metadata": {
        "id": "UE18VeyCaBFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = [\n",
        "    \"a\", \"b\", \"c\", \"d\",\n",
        "    \"e\",\n",
        "    \"a\",\n",
        "    \"a\", \"b\", \"c\", \"d\",\n",
        "    \"b\", \"c\", \"d\"\n",
        "]"
      ],
      "metadata": {
        "id": "lk43-bkSaLT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ALPHABET = set(training_data)"
      ],
      "metadata": {
        "id": "D4xhExDAaNJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"This is my alphabeth: {ALPHABET}\")\n",
        "print()\n",
        "print(f\"Its length is {len(ALPHABET)}\")"
      ],
      "metadata": {
        "id": "jyxLoGksaPI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_MERGES = 3"
      ],
      "metadata": {
        "id": "3c2UInJjaRdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "for _ in range(NUM_MERGES):\n",
        "    counter = Counter()\n",
        "\n",
        "    for cur_token, next_token in zip(training_data, training_data[1:]):\n",
        "        counter[cur_token + next_token] += 1\n",
        "\n",
        "    max_key = max(counter, key=counter.get)\n",
        "    print(f\"Found new max key: {max_key}!\")\n",
        "    ALPHABET.add(max_key)\n",
        "\n",
        "    training_data = update_training_data(training_data, max_key)"
      ],
      "metadata": {
        "id": "ifugljtFaTQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"This is my alphabeth: {ALPHABET}\")\n",
        "print()\n",
        "print(f\"Its length is {len(ALPHABET)}\")"
      ],
      "metadata": {
        "id": "4Yv4aT27aVyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 2: написать fine-tuning для языковой модели под набор данных:"
      ],
      "metadata": {
        "id": "99_zi1CR6F4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Описание датасета можно найти тут: \\\n",
        "https://paperswithcode.com/dataset/rucos \\\n",
        "https://huggingface.co/datasets/RussianNLP/russian_super_glue"
      ],
      "metadata": {
        "id": "3tEYC5MbGDmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"RussianNLP/russian_super_glue\", name='rucos')"
      ],
      "metadata": {
        "id": "wvVA4G8jbh_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RE_BAD_PATTERNS = re.compile(\"(@[a-z]+|\\n)\")"
      ],
      "metadata": {
        "id": "LtQ8I6d_KHBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_idx_from_train = random.randint(0, len(dataset['train']) - 1) # 66411\n",
        "\n",
        "random_object = dataset['train'][random_idx_from_train]['passage']\n",
        "\n",
        "filtered_random_object = RE_BAD_PATTERNS.sub(\" \", random_object)\n",
        "print(random_object)\n",
        "print(\"*\" * 20)\n",
        "print(filtered_random_object)"
      ],
      "metadata": {
        "id": "-s2lrzCB6DIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.add_special_tokens({'pad_token': \"<|endoftext|>\"})\n",
        "\n",
        "def texts_to_batch(texts: List[str]) -> torch.Tensor:\n",
        "    clean_texts = [\n",
        "        RE_BAD_PATTERNS.sub(\" \", _[\"passage\"]) for _ in texts\n",
        "    ]\n",
        "    tokenized_texts = tokenizer(\n",
        "        text=clean_texts,\n",
        "        return_tensors=\"pt\",\n",
        "        add_special_tokens=True,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True\n",
        "      )\n",
        "    return tokenized_texts"
      ],
      "metadata": {
        "id": "unuLI-A_Itji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 2\n",
        "\n",
        "train_dl = torch.utils.data.DataLoader(\n",
        "    dataset=dataset['train'],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    collate_fn=texts_to_batch\n",
        ")"
      ],
      "metadata": {
        "id": "HKyQJ5AvqXz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_ITERATIONS = 1000\n",
        "\n",
        "inputs = tokenizer(\"В прошлый четверг президенты Казахстана и России\", return_tensors=\"pt\")\n",
        "for k, v in inputs.items():\n",
        "  inputs[k] = v.to(DEVICE)"
      ],
      "metadata": {
        "id": "rnG7TkFTY2K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_SIZE = 40"
      ],
      "metadata": {
        "id": "cBNB57wmZC08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
        "llm_model.eval()\n",
        "llm_model.to(DEVICE)\n",
        "llm_outputs = llm_model(**inputs)"
      ],
      "metadata": {
        "id": "TXwBeXHkozD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(\n",
        "    params=llm_model.parameters(), lr=1e-6\n",
        ")\n",
        "\n",
        "cur_iteration = 0\n",
        "for batch in train_dl:\n",
        "    if cur_iteration == N_ITERATIONS:\n",
        "      break\n",
        "\n",
        "    llm_model.train()\n",
        "    #################\n",
        "    input_tokens = batch['input_ids'][:, :-1].to(DEVICE)\n",
        "    labels = batch['input_ids'].clone()[:, 1:].to(DEVICE)\n",
        "    attention_mask = batch['attention_mask'][:, :-1].to(DEVICE)\n",
        "    out_logits = llm_model(input_ids=input_tokens, attention_mask=attention_mask).logits\n",
        "    labels[labels == tokenizer.pad_token_id] = -100\n",
        "    loss_value = loss(out_logits.permute(0, 2, 1), labels)\n",
        "    #################\n",
        "    print(f\"Loss value: {loss_value.item()}\")\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    llm_model.eval()\n",
        "    for n_beams in range(2, 5):\n",
        "      beam_output = llm_model.generate(**inputs, max_new_tokens=OUTPUT_SIZE, num_beams=n_beams)\n",
        "      print(f\"Beam size={n_beams}\")\n",
        "      print(tokenizer.decode(beam_output[0], skip_special_tokens=True))\n",
        "      print()\n",
        "\n",
        "    print(\"*\" * 20)\n",
        "    cur_iteration += 1"
      ],
      "metadata": {
        "id": "w2fF7wjuYyHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 3: написать greedy search, сравнить результаты с имплементацией от transformers"
      ],
      "metadata": {
        "id": "fZ3gKx_SN8Td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_SIZE = 40"
      ],
      "metadata": {
        "id": "M5f0tj1ROp2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
        "llm_model.eval()\n",
        "llm_model.to(DEVICE)\n",
        "llm_outputs = llm_model(**inputs)"
      ],
      "metadata": {
        "id": "OdVqCO6YtNlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_expected_input(input_ids, attention_mask):\n",
        "    input_ids = torch.tensor(input_ids, device=DEVICE)\n",
        "    attention_mask = torch.tensor(attention_mask, device=DEVICE)\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask\n",
        "    }"
      ],
      "metadata": {
        "id": "Pir2YSvgOtIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, attention_mask = inputs[\"input_ids\"].tolist(), inputs[\"attention_mask\"].tolist()\n",
        "\"\"\"\n",
        "input_ids должно содержать нагенерированные токены\n",
        "\"\"\"\n",
        "\n",
        "for _ in range(OUTPUT_SIZE):\n",
        "  ######\n",
        "  bare_inputs = convert_to_expected_input(input_ids, attention_mask)\n",
        "  bare_outputs = bare_model(**bare_inputs, output_hidden_states=True)\n",
        "  logits = torch.matmul(\n",
        "      bare_outputs.last_hidden_state[-1][-1],\n",
        "      bare_model.wte.weight.T\n",
        "  )\n",
        "  bare_probas = F.softmax(logits, dim=0)\n",
        "  next_token = torch.argmax(bare_probas).item()\n",
        "  input_ids[-1].append(next_token)\n",
        "  attention_mask[-1].append(1)\n",
        "  ######"
      ],
      "metadata": {
        "id": "5c7URz8aOv39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_predictions = llm_model.generate(**inputs, max_new_tokens=OUTPUT_SIZE)"
      ],
      "metadata": {
        "id": "eseVteQSSVxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ниже проверяем, что наивная имплементация совпадает с ожидаемой:"
      ],
      "metadata": {
        "id": "36G_8mrjVCnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert input_ids[-1][-OUTPUT_SIZE:] == llm_predictions[-1][-OUTPUT_SIZE:].tolist()"
      ],
      "metadata": {
        "id": "9nR81p70TgJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(llm_predictions[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "HQOUB10sTisH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GMz7pt76auXd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}