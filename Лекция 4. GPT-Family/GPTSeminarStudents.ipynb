{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fwrjccfsem1U"},"outputs":[],"source":["!pip install datasets==2.16.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TDEtbNmJNP5i"},"outputs":[],"source":["import random\n","import re\n","from typing import List\n","\n","from datasets import load_dataset, list_datasets, Dataset\n","from IPython.display import Image\n","from IPython.core.display import HTML\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import transformers\n","from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2Model\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{"id":"N8Hla8m3wgjm"},"source":["https://huggingface.co/docs/transformers/model_doc/gpt2"]},{"cell_type":"markdown","metadata":{"id":"oJ51auF9rop7"},"source":["https://huggingface.co/docs/transformers/main_classes/tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G9nI6GwqNlSl"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ddvb-7swo6t4"},"outputs":[],"source":["DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","# DEVICE = torch.device(\"cpu\")\n","print(f\"Our device is {DEVICE}\")"]},{"cell_type":"markdown","metadata":{"id":"jstVDi-2Y-7d"},"source":["# Как работает генерация с т.з. кода под капотом:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"frkwEHCUVhIZ"},"outputs":[],"source":["TEXT_INPUT = \"Парламент- это не место для\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zDqkoPR9VfPx"},"outputs":[],"source":["inputs = tokenizer(TEXT_INPUT, return_tensors=\"pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ASX286Q2rb4G"},"outputs":[],"source":["for k, v in inputs.items():\n","  inputs[k] = v.to(DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OEAJfqmyiqWq"},"outputs":[],"source":["bare_model = GPT2Model.from_pretrained(\"gpt2\")\n","bare_model.eval()\n","bare_model.to(DEVICE)\n","bare_outputs = bare_model(**inputs, output_hidden_states=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HKT71AzAvbJ5"},"outputs":[],"source":["last_hidden_states = bare_outputs.last_hidden_state"]},{"cell_type":"markdown","metadata":{"id":"lBExneKSwz_J"},"source":["Weight tying"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IQl0LxZjwqfK"},"outputs":[],"source":["Image(url= \"https://lena-voita.github.io/resources/lectures/lang_models/practical/weight_tying_idea-min.png\", width=1900, height=900)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mezRRFrkjzTd"},"outputs":[],"source":["logits = torch.matmul(\n","    last_hidden_states[-1][-1],\n","    bare_model.wte.weight.T\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C6tdRDgsj8W-"},"outputs":[],"source":["bare_probas = F.softmax(logits, dim=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wZLo69g5j9VC"},"outputs":[],"source":["torch.argmax(bare_probas)"]},{"cell_type":"markdown","metadata":{"id":"xZSih2orj-ax"},"source":["# Упрощённая генерация:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tBcgTY12kV5D"},"outputs":[],"source":["llm_model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n","llm_model.eval()\n","llm_model.to(DEVICE)\n","llm_outputs = llm_model(**inputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DiWJ1qWXkk7H"},"outputs":[],"source":["torch.argmax(llm_outputs.logits[-1][-1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0GcBrRvrtBRZ"},"outputs":[],"source":["llm_probas = F.softmax(llm_outputs.logits[-1][-1], dim=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uw1dNB-RtVW7"},"outputs":[],"source":["torch.allclose(bare_probas, llm_probas, rtol=1e-4)"]},{"cell_type":"markdown","metadata":{"id":"CmhH8-dPZG9S"},"source":["# Задание 1: написать свою имплементацию BPE"]},{"cell_type":"markdown","metadata":{"id":"iOVbyjS0ZUUp"},"source":["Как работает алгоритм: \\\n","- У алгоритма один гиперпараметр- число итераций M \\\n","- На  i-ой итерации(1 <= i <= M) мы находим самую популярную пару токенов, идущих подряд \\(для примера наховём их a, b) \\\n","- Мы создаём новый токен, соответствующий конкатенции пары из предыдущего пункта \\(a,b -> ab), все вхождения пары в тренировочных данных заменяем на новый токен\n","- Возвращаемся к шагу 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lk43-bkSaLT8"},"outputs":[],"source":["training_data = [\n","    \"a\", \"b\", \"c\", \"d\",\n","    \"e\",\n","    \"a\",\n","    \"a\", \"b\", \"c\", \"d\",\n","    \"b\", \"c\", \"d\"\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D4xhExDAaNJQ"},"outputs":[],"source":["ALPHABET = set(training_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jyxLoGksaPI9"},"outputs":[],"source":["print(f\"This is my alphabeth: {ALPHABET}\")\n","print()\n","print(f\"Its length is {len(ALPHABET)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3c2UInJjaRdN"},"outputs":[],"source":["NUM_MERGES = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ifugljtFaTQe"},"outputs":[],"source":["\"\"\"\n","Your implementation here\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Yv4aT27aVyS"},"outputs":[],"source":["print(f\"This is my alphabeth: {ALPHABET}\")\n","print()\n","print(f\"Its length is {len(ALPHABET)}\")"]},{"cell_type":"markdown","metadata":{"id":"99_zi1CR6F4r"},"source":["# Задание 2: написать fine-tuning для языковой модели под набор данных:"]},{"cell_type":"markdown","metadata":{"id":"3tEYC5MbGDmJ"},"source":["Описание датасета можно найти тут: \\\n","https://paperswithcode.com/dataset/rucos \\\n","https://huggingface.co/datasets/RussianNLP/russian_super_glue"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wvVA4G8jbh_R"},"outputs":[],"source":["dataset = load_dataset(\"RussianNLP/russian_super_glue\", name='rucos')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LtQ8I6d_KHBD"},"outputs":[],"source":["RE_BAD_PATTERNS = re.compile(\"(@[a-z]+|\\n)\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-s2lrzCB6DIz"},"outputs":[],"source":["random_idx_from_train = random.randint(0, len(dataset['train']) - 1)\n","\n","random_object = dataset['train'][random_idx_from_train]['passage']\n","\n","filtered_random_object = RE_BAD_PATTERNS.sub(\" \", random_object)\n","print(random_object)\n","print(\"*\" * 20)\n","print(filtered_random_object)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"unuLI-A_Itji"},"outputs":[],"source":["tokenizer.add_special_tokens({'pad_token': \"<|endoftext|>\"})\n","\n","def texts_to_batch(texts: List[str]) -> torch.Tensor:\n","    clean_texts = [\n","        RE_BAD_PATTERNS.sub(\" \", _[\"passage\"]) for _ in texts\n","    ]\n","    batch = tokenizer(\n","        text=clean_texts,\n","        return_tensors=\"pt\",\n","        add_special_tokens=True,\n","        padding=\"max_length\",\n","        truncation=True\n","      )\n","    return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HKyQJ5AvqXz-"},"outputs":[],"source":["BATCH_SIZE = 2\n","\n","train_dl = torch.utils.data.DataLoader(\n","    dataset=dataset['train'],\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    drop_last=True,\n","    collate_fn=texts_to_batch\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rnG7TkFTY2K7"},"outputs":[],"source":["N_ITERATIONS = 1000\n","\n","inputs = tokenizer(\"В прошлый четверг президенты Казахстана и России\", return_tensors=\"pt\")\n","for k, v in inputs.items():\n","  inputs[k] = v.to(DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cBNB57wmZC08"},"outputs":[],"source":["OUTPUT_SIZE = 40"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4RIoQ3rdYv3p"},"outputs":[],"source":["llm_model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n","llm_model.eval()\n","llm_model.to(DEVICE)\n","llm_outputs = llm_model(**inputs)"]},{"cell_type":"markdown","metadata":{"id":"b9s_pMCHZFLP"},"source":["Подумайте о том:\n","- как формировать входной батч токенов и attention_masks для модели\n","- как формировать батч лейблов\n","- как будете считать лосс"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w2fF7wjuYyHb"},"outputs":[],"source":["loss = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(\n","    params=llm_model.parameters(), lr=1e-6\n",")\n","\n","cur_iteration = 0\n","for batch in train_dl:\n","    if cur_iteration == N_ITERATIONS:\n","      break\n","\n","    llm_model.train()\n","    \"\"\"\n","    Your implementation here\n","    \"\"\"\n","    loss_value = None  # Write your loss computation here and after each batch print it to ensure that the loss decreases\n","    print(f\"Loss value: {loss_value.item()}\")\n","    loss_value.backward()\n","    optimizer.step()\n","\n","\n","    \"\"\"\n","    After the batch consumption, see how your model performs to ensure that it's getting better at generation\n","    \"\"\"\n","    llm_model.eval()\n","    for n_beams in range(2, 5):\n","      beam_output = llm_model.generate(**inputs, max_new_tokens=OUTPUT_SIZE, num_beams=n_beams)\n","      print(f\"Beam size={n_beams}\")\n","      print(tokenizer.decode(beam_output[0], skip_special_tokens=True))\n","      print()\n","\n","    print(\"*\" * 20)\n","    cur_iteration += 1"]},{"cell_type":"markdown","metadata":{"id":"fZ3gKx_SN8Td"},"source":["# Задание 3: написать greedy search, сравнить результаты с имплементацией от transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M5f0tj1ROp2W"},"outputs":[],"source":["OUTPUT_SIZE = 40"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OdVqCO6YtNlC"},"outputs":[],"source":["llm_model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n","llm_model.eval()\n","llm_model.to(DEVICE)\n","llm_outputs = llm_model(**inputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pir2YSvgOtIZ"},"outputs":[],"source":["def convert_to_expected_input(input_ids, attention_mask):\n","    input_ids = torch.tensor(input_ids, device=DEVICE)\n","    attention_mask = torch.tensor(attention_mask, device=DEVICE)\n","    return {\n","        \"input_ids\": input_ids,\n","        \"attention_mask\": attention_mask\n","    }"]},{"cell_type":"markdown","metadata":{"id":"mBZpulqfaz7q"},"source":["Напомним, как работает greeedy search:\n","- на каждой итерации i по последовательности токенов s мы делаем предсказание и берём максимальный по вероятности токен\n","- он конкатенируется к s\n","- возвращаемся в п.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5c7URz8aOv39"},"outputs":[],"source":["\"\"\"\n","Note, that input_ids and attention_mask are LISTS!\n","input_ids should contain the tokens you predict after each iteration\n","\"\"\"\n","input_ids, attention_mask = inputs[\"input_ids\"].tolist(), inputs[\"attention_mask\"].tolist()\n","\n","for _ in range(OUTPUT_SIZE):\n","  \"\"\"\n","  Your implementation here\n","  \"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eseVteQSSVxK"},"outputs":[],"source":["llm_predictions = llm_model.generate(**inputs, max_new_tokens=OUTPUT_SIZE)"]},{"cell_type":"markdown","metadata":{"id":"36G_8mrjVCnK"},"source":["Ниже проверяем, что наивная имплементация совпадает с ожидаемой:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9nR81p70TgJx"},"outputs":[],"source":["assert input_ids[-1][-OUTPUT_SIZE:] == llm_predictions[-1][-OUTPUT_SIZE:].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HQOUB10sTisH"},"outputs":[],"source":["tokenizer.decode(llm_predictions[0], skip_special_tokens=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GMz7pt76auXd"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMlOBfvfOr+Ou9cwJUISyEA","gpuType":"T4","provenance":[{"file_id":"1XyXu0E0MHxOQDo2vhU_njksAthVWJMuL","timestamp":1709034050213}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
