{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e295fedf",
   "metadata": {},
   "source": [
    "# Практика 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740b61e0",
   "metadata": {},
   "source": [
    "В этом задании необходимо будет разобраться в jupyter-ноутбуке по темам первой лекции, запустить / дописать пару строк кода и ответить на несложные вопросы. Цель ноутбука - показать базовые принципы работы с текстовыми эмбеддингами, построением задачи языкового моделирования и проблемой борьбы за уменьшение количество словаря / параметров в нейронных сетях.\n",
    "\n",
    "Также, этот код будет полезен в качестве основы для обучения собственных персональных ассистентов в рамках дополнительных заданий. \n",
    "\n",
    "Откуда брать ноутбук: https://github.com/dmkalash/mailru_llm_course/tree/main/Лекция%201.%20Введение%20в%20NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c216ee56",
   "metadata": {},
   "source": [
    "Стоит учитывать, что в некоторых частях данного ноутбука я заведомо пожертвовал оптимальностью ради большей прозрачности кода и лучшего понимания базовых принципов работы с эмбеддингами и языковым моделированием. Как оптимизировать те или иные моменты, мы будем рассказывать в следующих лекциях и семинарах."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a8e63f",
   "metadata": {},
   "source": [
    "## Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893205d4",
   "metadata": {},
   "source": [
    "Посмотрим, как работает векторная арифметика в подходе fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acdbaf77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-22T09:50:06.576739Z",
     "iopub.status.busy": "2024-02-22T09:50:06.576214Z",
     "iopub.status.idle": "2024-02-22T09:50:35.946713Z",
     "shell.execute_reply": "2024-02-22T09:50:35.945952Z",
     "shell.execute_reply.started": "2024-02-22T09:50:06.576693Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle \n",
    "from itertools import chain\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "from typing import List, Dict, Optional, Iterable, Tuple\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0934d9fb",
   "metadata": {},
   "source": [
    "## Датасет, с которым будем работать. \n",
    "\n",
    "Берем небольшое количество обучающих примеров, чтобы эксперименты можно было проводить быстро. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f779cdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T14:50:37.191863Z",
     "iopub.status.busy": "2024-02-21T14:50:37.190428Z",
     "iopub.status.idle": "2024-02-21T14:50:37.205992Z",
     "shell.execute_reply": "2024-02-21T14:50:37.204512Z",
     "shell.execute_reply.started": "2024-02-21T14:50:37.191816Z"
    }
   },
   "outputs": [],
   "source": [
    "# ds_name_2 = 'IlyaGusev/stihi_ru'\n",
    "\n",
    "def get_dataset(train_size: int,\n",
    "                test_size: int,\n",
    "                ds_name_1: str = 'IlyaGusev/gazeta',\n",
    "               ): \n",
    "    \n",
    "    train_dataset = load_dataset(ds_name_1, split='train')\n",
    "    test_dataset = load_dataset(ds_name_1, split='test')\n",
    "\n",
    "    train_df = pd.DataFrame(train_dataset).iloc[:train_size]\n",
    "    print(train_df.shape)\n",
    "\n",
    "    test_df = pd.DataFrame(test_dataset)[:test_size]\n",
    "    print(test_df.shape)\n",
    "\n",
    "    train_texts = (train_df['title'] + '\\n' + train_df['text']).tolist()\n",
    "    test_texts = (test_df['title'] + '\\n' + test_df['text']).tolist()\n",
    "    \n",
    "    return train_texts, test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f389d92e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T14:50:37.209349Z",
     "iopub.status.busy": "2024-02-21T14:50:37.207871Z",
     "iopub.status.idle": "2024-02-21T14:51:18.161828Z",
     "shell.execute_reply": "2024-02-21T14:51:18.160746Z",
     "shell.execute_reply.started": "2024-02-21T14:50:37.209314Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oplora/Documents/VMK/VK_LLM/nlp/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for IlyaGusev/gazeta contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/IlyaGusev/gazeta\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 5)\n",
      "(500, 5)\n"
     ]
    }
   ],
   "source": [
    "train_texts, test_texts = get_dataset(5000, 500)\n",
    "all_texts = train_texts + test_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf91bfb",
   "metadata": {},
   "source": [
    "## Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7c2e51",
   "metadata": {},
   "source": [
    "Посмотрим, как получать эмбеддинги текстов с помощью fasttext, и как находить похожие тексты. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70203b02",
   "metadata": {},
   "source": [
    "Эту часть задания советую выполнять локально на ноутбуке. В Kaggle-ноутбуке из-за интернета предобученный fasttext загружается слишком медленно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4df6d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fasttext==0.9.2\n",
    "\n",
    "import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d18144f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cc.ru.300.bin'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext.util.download_model('ru', if_exists='ignore')  # Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0841ae9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft = fasttext.load_model('cc.ru.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "862cd305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "«Чикаго» в шаге от Кубка Стэнли\n",
      "После двух поражений в Чикаго «летчикам» на своей площадке удалось отыграться, так что пятого матча все ждали с нетерпением. Его победитель заметно повышал свои шансы н \n",
      "\n",
      "«Будут бить, если что»\n",
      "В ночь на понедельник резко обострилась ситуация в Кадашах, где уже третью неделю идет акция в защиту исторического наследия города. Как сообщил координатор «Московского совета» \n",
      "\n"
     ]
    }
   ],
   "source": [
    "anchor_ind = 120\n",
    "print(train_texts[anchor_ind][:200], '\\n')\n",
    "print(train_texts[anchor_ind + 3][:200], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7fa83e",
   "metadata": {},
   "source": [
    "Для нахождения похожих текстов по векторам как правило используют косинусную близость, ее можно реализовать самому, можно воспользоваться готовой реализацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "028b269b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9070907876494414\n",
      "0.9070907\n"
     ]
    }
   ],
   "source": [
    "v1 = ft.get_sentence_vector(train_texts[anchor_ind].replace('\\n', ' '))\n",
    "v2 = ft.get_sentence_vector(train_texts[anchor_ind + 3].replace('\\n', ' '))\n",
    "\n",
    "# K(X, Y) = <X, Y> / (||X||*||Y||)\n",
    "\n",
    "print( (v1 * v2).sum() / (v1 ** 2).sum() ** 0.5 / (v2 ** 2).sum() ** 0.5 )\n",
    "print( cosine_similarity(v1.reshape(1, -1), v2.reshape(1, -1))[0][0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7a1a2d",
   "metadata": {},
   "source": [
    "#### >>> Задание 1\n",
    "\n",
    "Найдите в корпусе тренировочных текстов train_texts текст, наиболее похожий по косинусному расстоянию на train_texts[anchor_ind], где anchor_ind=50. В качестве ответа выведите похожесть данных текстов (число напротив \"Похожесть:\") \n",
    "\n",
    "Для всех текстов используйте единственную нормализацию как в примере выше: .replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "910db380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Номер текста 255 \n",
      " Похожесть:  0.92 \n",
      " Текст:  «Брешия» вернулась в элиту\n",
      "Вряд ли можно было предсказать обладателя последней путевки в серию А после первого финального матча. Ведь даже нулевая ничья в Турине не давала преимущества ни одной из команд. К слову, в нынешнем сезоне «бьянкоадзурри» уже обыгрывали «Торино» на своем поле (1:0). На пути\n"
     ]
    }
   ],
   "source": [
    "anchor_ind = 120\n",
    "need_length = 300\n",
    "\n",
    "v1 = ft.get_sentence_vector(train_texts[anchor_ind][:need_length].replace('\\n', ' '))\n",
    "\n",
    "sim_prev = 0\n",
    "for ind, text in (enumerate(train_texts)):\n",
    "    \n",
    "    v = ft.get_sentence_vector(text[:need_length].replace('\\n', ' '))\n",
    "    sim_curr = cosine_similarity(v1.reshape(1, -1), v.reshape(1, -1))[0][0]\n",
    "    \n",
    "    if sim_curr > sim_prev and ind != anchor_ind:\n",
    "        sim_prev = sim_curr\n",
    "        max_ind = ind\n",
    "\n",
    "    \n",
    "max_similarity = sim_prev\n",
    "max_sim_text = train_texts[max_ind]\n",
    "\n",
    "\n",
    "print(\n",
    "    'Номер текста', max_ind, '\\n',\n",
    "    'Похожесть: ', round(max_similarity, 3), '\\n',\n",
    "    'Текст: ', max_sim_text[:need_length]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2b1004",
   "metadata": {},
   "source": [
    "В качестве ответа укажите выведенную похожесть в коде выше с той же точностью. Например: 0.395"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b869a9b",
   "metadata": {},
   "source": [
    "$\\textbf{Ответ}:\\text{Похожесть для anchor_ind = 50} - 0.885$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef92972",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-22T09:57:53.280890Z",
     "iopub.status.busy": "2024-02-22T09:57:53.279793Z",
     "iopub.status.idle": "2024-02-22T09:57:53.287397Z",
     "shell.execute_reply": "2024-02-22T09:57:53.286293Z",
     "shell.execute_reply.started": "2024-02-22T09:57:53.280855Z"
    }
   },
   "source": [
    "Как мы видим, оба текста про спорт, пусть и содержат различную семантику. Если взять anchor_ind=12, то можно обнаружить в датасете дубликат - этого стоит избегать при обучении моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7527eee0",
   "metadata": {},
   "source": [
    "## LM на основе n-грамм"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c401596",
   "metadata": {},
   "source": [
    "### 1 этап - предобработка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ab31d2",
   "metadata": {},
   "source": [
    "В качестве предобработки сделаем следующее:\n",
    "* приведем все тексты к нижнему регистру\n",
    "* унифицируем все пробельные символы\n",
    "\n",
    "Напишите код для приведения всех слов в тексте к нижнему регистру в методе text_preprocess ниже"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8821f07a",
   "metadata": {},
   "source": [
    "### 2 этап - токенизация, составление словаря"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3749d827",
   "metadata": {},
   "source": [
    "Выделим все слова в корпусе с помощью регекса, а также добавим туда базовые знаки пунктуации и пробел"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15cc0227",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:11.789756Z",
     "iopub.status.busy": "2024-02-21T15:03:11.788899Z",
     "iopub.status.idle": "2024-02-21T15:03:11.808584Z",
     "shell.execute_reply": "2024-02-21T15:03:11.807519Z",
     "shell.execute_reply.started": "2024-02-21T15:03:11.789725Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self,\n",
    "                 token_pattern: str = '\\w+|[\\!\\?\\,\\.\\-\\:]',\n",
    "                 eos_token: str = '<EOS>',\n",
    "                 pad_token: str = '<PAD>',\n",
    "                 unk_token: str = '<UNK>',\n",
    "                 lower = True):\n",
    "        self.token_pattern = token_pattern\n",
    "        self.eos_token = eos_token\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        self.lower = lower\n",
    "        \n",
    "        self.special_tokens = [self.eos_token, self.pad_token, self.unk_token]\n",
    "        self.vocab = None\n",
    "        self.inverse_vocab = None\n",
    "    \n",
    "    def text_preprocess(self, input_text: str) -> str:\n",
    "        \"\"\" Предобрабатываем один текст \"\"\"\n",
    "        input_text = input_text.lower() if self.lower else input_text # приведение к нижнему регистру\n",
    "        input_text = re.sub('\\s+', ' ', input_text) # унифицируем пробелы\n",
    "        input_text = input_text.strip()\n",
    "        return input_text\n",
    "    \n",
    "    def build_vocab(self, corpus: List[str]) -> None:\n",
    "        assert len(corpus)\n",
    "        all_tokens = set()\n",
    "        for text in corpus:\n",
    "            all_tokens |= set(self._tokenize(text, append_eos_token=False))\n",
    "        self.vocab = {elem: ind for ind, elem in enumerate(all_tokens)}\n",
    "        special_tokens = [self.eos_token, self.unk_token, self.pad_token]\n",
    "        for token in special_tokens:\n",
    "            self.vocab[token] = len(self.vocab) + 1\n",
    "        self.inverse_vocab = {ind: elem for elem, ind in self.vocab.items()}\n",
    "        return self\n",
    "        \n",
    "    def _tokenize(self, text: str, append_eos_token: bool = True) -> List[str]:\n",
    "        text = self.text_preprocess(text)\n",
    "        tokens = re.findall(self.token_pattern, text)\n",
    "        if append_eos_token:\n",
    "            tokens.append(self.eos_token)\n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, text: str, append_eos_token: bool = True) -> List[str]:\n",
    "        \"\"\" Токенизируем текст \"\"\"\n",
    "        tokens = self._tokenize(text, append_eos_token)\n",
    "        ids = [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, input_ids: Iterable[int], remove_special_tokens: bool = False) -> str:\n",
    "        assert len(input_ids)\n",
    "        assert max(input_ids) < len(self.vocab) and min(input_ids) >= 0\n",
    "        tokens = []\n",
    "        for ind in input_ids:\n",
    "            token = self.inverse_vocab[ind]\n",
    "            if remove_special_tokens and token in self.special_tokens:\n",
    "                continue\n",
    "            tokens.append(token)\n",
    "        text = ' '.join( tokens )\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a28362a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:14.208673Z",
     "iopub.status.busy": "2024-02-21T15:03:14.208003Z",
     "iopub.status.idle": "2024-02-21T15:03:14.213372Z",
     "shell.execute_reply": "2024-02-21T15:03:14.212422Z",
     "shell.execute_reply.started": "2024-02-21T15:03:14.208640Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer().build_vocab(['вот такие прироги и ничего больше', '! ? . '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48bc1f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'такие прироги и ничего?!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dd4b8b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:15.781116Z",
     "iopub.status.busy": "2024-02-21T15:03:15.780372Z",
     "iopub.status.idle": "2024-02-21T15:03:15.787896Z",
     "shell.execute_reply": "2024-02-21T15:03:15.787061Z",
     "shell.execute_reply.started": "2024-02-21T15:03:15.781081Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'прироги': 0,\n",
       " '?': 1,\n",
       " 'такие': 2,\n",
       " 'больше': 3,\n",
       " 'ничего': 4,\n",
       " '!': 5,\n",
       " '.': 6,\n",
       " 'вот': 7,\n",
       " 'и': 8,\n",
       " '<EOS>': 10,\n",
       " '<UNK>': 11,\n",
       " '<PAD>': 12}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8edc6f35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:17.513966Z",
     "iopub.status.busy": "2024-02-21T15:03:17.513587Z",
     "iopub.status.idle": "2024-02-21T15:03:17.520422Z",
     "shell.execute_reply": "2024-02-21T15:03:17.519488Z",
     "shell.execute_reply.started": "2024-02-21T15:03:17.513937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0, 8, 4, 1, 5, 10]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77f763a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:17.744830Z",
     "iopub.status.busy": "2024-02-21T15:03:17.744433Z",
     "iopub.status.idle": "2024-02-21T15:03:17.751247Z",
     "shell.execute_reply": "2024-02-21T15:03:17.750269Z",
     "shell.execute_reply.started": "2024-02-21T15:03:17.744801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['такие', 'прироги', 'и', 'ничего', '?', '!']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer._tokenize(text, append_eos_token=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cee40ac3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:19.193337Z",
     "iopub.status.busy": "2024-02-21T15:03:19.192389Z",
     "iopub.status.idle": "2024-02-21T15:03:19.199497Z",
     "shell.execute_reply": "2024-02-21T15:03:19.198313Z",
     "shell.execute_reply.started": "2024-02-21T15:03:19.193304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'такие прироги и ничего ? ! <EOS>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text), remove_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f415f00e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:19.450225Z",
     "iopub.status.busy": "2024-02-21T15:03:19.449945Z",
     "iopub.status.idle": "2024-02-21T15:03:19.456301Z",
     "shell.execute_reply": "2024-02-21T15:03:19.455272Z",
     "shell.execute_reply.started": "2024-02-21T15:03:19.450202Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'такие прироги и ничего ? !'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text), remove_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f973864",
   "metadata": {},
   "source": [
    "### 3 этап - LM на основе n-грамм"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302067be",
   "metadata": {},
   "source": [
    "Напишем класс, который будет делать языковое моделирование на основе n-грамм. Изучите код, посмотрите какие функции здесь за что отвечают.\n",
    "\n",
    "На лекции обсуждали сглаживание Лапласа для этой модели. В методе _ get_next_token определяются статистики для рассматриваемых n-грамм. В этом коде отсутствует сглаживание Лапласа. Добавьте его, согласно формуле из презентации.\n",
    "\n",
    "К слову, если этого не сделать, то генерация будет завершаться с ошибкой - тк статистики по некоторым текстам нет, будем делить на ноль."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a047d647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:21.268282Z",
     "iopub.status.busy": "2024-02-21T15:03:21.267567Z",
     "iopub.status.idle": "2024-02-21T15:03:21.292501Z",
     "shell.execute_reply": "2024-02-21T15:03:21.291323Z",
     "shell.execute_reply.started": "2024-02-21T15:03:21.268251Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class StatLM:\n",
    "    def __init__(self, \n",
    "                 #vocab: Dict[str, int], \n",
    "                 tokenizer: Tokenizer,\n",
    "                 context_size: int = 2,\n",
    "                 alpha: float = 0.1,\n",
    "                 sample_top_p: Optional[float] = None\n",
    "                ):\n",
    "        \n",
    "        assert context_size >= 2\n",
    "        assert sample_top_p is None or 0.0 < sample_top_p <= 1.0\n",
    "        \n",
    "        self.context_size = context_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.alpha = alpha\n",
    "        self.sample_top_p = sample_top_p\n",
    "        \n",
    "        self.n_gramms_stat = defaultdict(int)\n",
    "        self.nx_gramms_stat = defaultdict(int)\n",
    "        \n",
    "    def get_token_by_ind(ind: int) -> str:\n",
    "        return self.tokenizer.vocab.get(ind)\n",
    "    \n",
    "    def get_ind_by_token(token: str) -> int:\n",
    "        return self.tokenizer.inverse_vocab.get(token, self.tokenizer.inverse_vocab[self.unk_token])\n",
    "        \n",
    "    #def train(self, train_token_indices: List[List[int]]):\n",
    "    def train(self, train_texts: List[str]):\n",
    "        for sentence in tqdm(train_texts, desc='train lines'):\n",
    "            sentence_ind = self.tokenizer.encode(sentence)\n",
    "            for i in range(len(sentence_ind) - self.context_size):\n",
    "                \n",
    "                seq = tuple(sentence_ind[i: i + self.context_size - 1])\n",
    "                self.n_gramms_stat[seq] += 1\n",
    "                \n",
    "                seq_x = tuple(sentence_ind[i: i + self.context_size])\n",
    "                self.nx_gramms_stat[seq_x] += 1\n",
    "                \n",
    "            seq = tuple(sentence_ind[len(sentence_ind) - self.context_size:])\n",
    "            self.n_gramms_stat[seq] += 1\n",
    "            \n",
    "    def sample_token(self, token_distribution: np.ndarray) -> int:\n",
    "        if self.sample_top_p is None:\n",
    "            return token_distribution.argmax()\n",
    "        else:\n",
    "            token_distribution = sorted(list(zip(token_distribution, np.arange(len(token_distribution)))))\n",
    "            total_proba = 0.0\n",
    "            tokens_to_sample = []\n",
    "            tokens_probas = []\n",
    "            for token_proba, ind in sorted(token_distribution, reverse=True):\n",
    "                tokens_to_sample.append(ind)\n",
    "                tokens_probas.append(token_proba)\n",
    "                total_proba += token_proba\n",
    "                if total_proba >= self.sample_top_p:\n",
    "                    break\n",
    "            # для простоты отнормируем вероятности, чтобы суммировались в единицу\n",
    "            tokens_probas = np.array(tokens_probas)\n",
    "            tokens_probas = tokens_probas / tokens_probas.sum()\n",
    "            return np.random.choice(tokens_to_sample, p=tokens_probas)\n",
    "        \n",
    "    def get_stat(self) -> Dict[str, Dict]:\n",
    "        \n",
    "        n_token_stat, nx_token_stat = {}, {}\n",
    "        for token_inds, count in self.n_gramms_stat.items():\n",
    "            n_token_stat[self.tokenizer.decode(token_inds)] = count\n",
    "        \n",
    "        for token_inds, count in self.nx_gramms_stat.items():\n",
    "            nx_token_stat[self.tokenizer.decode(token_inds)] = count\n",
    "        \n",
    "        return {\n",
    "            'n gramms stat': self.n_gramms_stat,\n",
    "            'n+1 gramms stat': self.nx_gramms_stat,\n",
    "            'n tokens stat': n_token_stat,\n",
    "            'n+1 tokens stat': nx_token_stat,\n",
    "        }\n",
    "    \n",
    "    def _get_next_token(self, tokens: List[int]) -> (int, str):\n",
    "        denominator = self.n_gramms_stat.get(tuple(tokens), 0) + 1 # TODO: сглаживание Лапласа\n",
    "        numerators = []\n",
    "        for ind in self.tokenizer.inverse_vocab:\n",
    "            numerators.append(self.nx_gramms_stat.get(tuple(tokens + [ind]), 0) + 1) # TODO: сглаживание Лапласа\n",
    "        \n",
    "        token_distribution = np.array(numerators) / denominator\n",
    "        max_proba_ind = self.sample_token(token_distribution)\n",
    "        \n",
    "        next_token = self.tokenizer.inverse_vocab[max_proba_ind]\n",
    "        \n",
    "        return max_proba_ind, next_token\n",
    "            \n",
    "    def generate_token(self, text: str, remove_special_tokens: bool = False) -> Dict:\n",
    "        tokens = self.tokenizer.encode(text, append_eos_token=False)\n",
    "        tokens = tokens[-self.context_size + 1:]\n",
    "        \n",
    "        max_proba_ind, next_token = self._get_next_token(tokens)\n",
    "        \n",
    "        return {\n",
    "            'next_token': next_token,\n",
    "            'next_token_num': max_proba_ind,\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def generate_text(self, text: str, max_tokens: int, remove_special_tokens: bool = False) -> Dict:\n",
    "        all_tokens = self.tokenizer.encode(text, append_eos_token=False)\n",
    "        tokens = all_tokens[-self.context_size + 1:]\n",
    "        \n",
    "        next_token = None\n",
    "        while next_token != self.tokenizer.eos_token and len(all_tokens) < max_tokens:\n",
    "            max_proba_ind, next_token = self._get_next_token(tokens)\n",
    "            all_tokens.append(max_proba_ind)\n",
    "            tokens = all_tokens[-self.context_size + 1:]\n",
    "        \n",
    "        new_text = self.tokenizer.decode(all_tokens, remove_special_tokens)\n",
    "        \n",
    "        finish_reason = 'max tokens'\n",
    "        if all_tokens[-1] == self.tokenizer.vocab[self.tokenizer.eos_token]:\n",
    "            finish_reason = 'end of text'\n",
    "        \n",
    "        return {\n",
    "            'all_tokens': all_tokens,\n",
    "            'total_text': new_text,\n",
    "            'finish_reason': finish_reason\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7161b5",
   "metadata": {},
   "source": [
    "Для демонстрации того, что происходит, возьмем несколько коротких цитат Джейсона Стэтхема отсюда:\n",
    "\n",
    "https://dzen.ru/a/ZRFaGN_gKhX6xTWW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6840e716",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:23.642385Z",
     "iopub.status.busy": "2024-02-21T15:03:23.641563Z",
     "iopub.status.idle": "2024-02-21T15:03:23.647640Z",
     "shell.execute_reply": "2024-02-21T15:03:23.646711Z",
     "shell.execute_reply.started": "2024-02-21T15:03:23.642338Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'Взял нож - режь, взял дошик - ешь.',\n",
    "    'Никогда не сдавайтесь, идите к своей цели! А если будет сложно – сдавайтесь.',\n",
    "    'Запомни: всего одна ошибка – и ты ошибся.',\n",
    "    'В жизни всегда есть две дороги: одна — первая, а другая — вторая.',\n",
    "    'Делай, как надо. Как не надо, не делай.',\n",
    "    'Работа не волк. Никто не волк. Только волк волк.',\n",
    "    'Работа не волк. Работа - ворк. А волк - это ходить.',\n",
    "    'Работа',\n",
    "    ]\n",
    "\n",
    "train_texts = texts[:-1]\n",
    "test_text = texts[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2085c59d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:25.218697Z",
     "iopub.status.busy": "2024-02-21T15:03:25.218225Z",
     "iopub.status.idle": "2024-02-21T15:03:25.223271Z",
     "shell.execute_reply": "2024-02-21T15:03:25.222261Z",
     "shell.execute_reply.started": "2024-02-21T15:03:25.218665Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer().build_vocab(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a52a63f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:25.398291Z",
     "iopub.status.busy": "2024-02-21T15:03:25.397981Z",
     "iopub.status.idle": "2024-02-21T15:03:25.406154Z",
     "shell.execute_reply": "2024-02-21T15:03:25.405134Z",
     "shell.execute_reply.started": "2024-02-21T15:03:25.398265Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'идите': 0,\n",
       " 'режь': 1,\n",
       " 'две': 2,\n",
       " 'делай': 3,\n",
       " 'работа': 4,\n",
       " 'ошибка': 5,\n",
       " 'одна': 6,\n",
       " 'если': 7,\n",
       " 'никто': 8,\n",
       " 'жизни': 9,\n",
       " 'дороги': 10,\n",
       " 'как': 11,\n",
       " 'а': 12,\n",
       " 'дошик': 13,\n",
       " '-': 14,\n",
       " 'сложно': 15,\n",
       " 'взял': 16,\n",
       " 'первая': 17,\n",
       " 'всего': 18,\n",
       " 'ошибся': 19,\n",
       " 'к': 20,\n",
       " 'надо': 21,\n",
       " 'сдавайтесь': 22,\n",
       " 'не': 23,\n",
       " 'ходить': 24,\n",
       " 'есть': 25,\n",
       " 'своей': 26,\n",
       " 'вторая': 27,\n",
       " 'нож': 28,\n",
       " 'никогда': 29,\n",
       " 'ворк': 30,\n",
       " 'цели': 31,\n",
       " '!': 32,\n",
       " 'ешь': 33,\n",
       " 'и': 34,\n",
       " 'другая': 35,\n",
       " 'это': 36,\n",
       " ':': 37,\n",
       " 'в': 38,\n",
       " ',': 39,\n",
       " 'только': 40,\n",
       " '.': 41,\n",
       " 'запомни': 42,\n",
       " 'ты': 43,\n",
       " 'будет': 44,\n",
       " 'всегда': 45,\n",
       " 'волк': 46,\n",
       " '<EOS>': 48,\n",
       " '<UNK>': 49,\n",
       " '<PAD>': 50}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2a843e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:29.936948Z",
     "iopub.status.busy": "2024-02-21T15:03:29.936563Z",
     "iopub.status.idle": "2024-02-21T15:03:29.957657Z",
     "shell.execute_reply": "2024-02-21T15:03:29.956641Z",
     "shell.execute_reply.started": "2024-02-21T15:03:29.936919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf0ffef97a446cc800ad2527741b3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train lines:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# класс, который позволяем строить и использовать языковую модель на основе n-грамм\n",
    "stat_lm = StatLM(tokenizer, context_size=2, alpha=0.1, sample_top_p = None)\n",
    "\n",
    "# \"обучаем\" модель - считаем статистики\n",
    "stat_lm.train(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "455d9035",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:32.262952Z",
     "iopub.status.busy": "2024-02-21T15:03:32.262566Z",
     "iopub.status.idle": "2024-02-21T15:03:32.273912Z",
     "shell.execute_reply": "2024-02-21T15:03:32.272879Z",
     "shell.execute_reply.started": "2024-02-21T15:03:32.262923Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['n gramms stat', 'n+1 gramms stat', 'n tokens stat', 'n+1 tokens stat'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'взял нож': 1,\n",
       " 'нож -': 1,\n",
       " '- режь': 1,\n",
       " 'режь ,': 1,\n",
       " ', взял': 1,\n",
       " 'взял дошик': 1,\n",
       " 'дошик -': 1,\n",
       " '- ешь': 1,\n",
       " 'ешь .': 1,\n",
       " 'никогда не': 1,\n",
       " 'не сдавайтесь': 1,\n",
       " 'сдавайтесь ,': 1,\n",
       " ', идите': 1,\n",
       " 'идите к': 1,\n",
       " 'к своей': 1,\n",
       " 'своей цели': 1,\n",
       " 'цели !': 1,\n",
       " '! а': 1,\n",
       " 'а если': 1,\n",
       " 'если будет': 1,\n",
       " 'будет сложно': 1,\n",
       " 'сложно сдавайтесь': 1,\n",
       " 'сдавайтесь .': 1,\n",
       " 'запомни :': 1,\n",
       " ': всего': 1,\n",
       " 'всего одна': 1,\n",
       " 'одна ошибка': 1,\n",
       " 'ошибка и': 1,\n",
       " 'и ты': 1,\n",
       " 'ты ошибся': 1,\n",
       " 'ошибся .': 1,\n",
       " 'в жизни': 1,\n",
       " 'жизни всегда': 1,\n",
       " 'всегда есть': 1,\n",
       " 'есть две': 1,\n",
       " 'две дороги': 1,\n",
       " 'дороги :': 1,\n",
       " ': одна': 1,\n",
       " 'одна первая': 1,\n",
       " 'первая ,': 1,\n",
       " ', а': 1,\n",
       " 'а другая': 1,\n",
       " 'другая вторая': 1,\n",
       " 'вторая .': 1,\n",
       " 'делай ,': 1,\n",
       " ', как': 1,\n",
       " 'как надо': 1,\n",
       " 'надо .': 1,\n",
       " '. как': 1,\n",
       " 'как не': 1,\n",
       " 'не надо': 1,\n",
       " 'надо ,': 1,\n",
       " ', не': 1,\n",
       " 'не делай': 1,\n",
       " 'делай .': 1,\n",
       " 'работа не': 2,\n",
       " 'не волк': 3,\n",
       " 'волк .': 4,\n",
       " '. никто': 1,\n",
       " 'никто не': 1,\n",
       " '. только': 1,\n",
       " 'только волк': 1,\n",
       " 'волк волк': 1,\n",
       " '. работа': 1,\n",
       " 'работа -': 1,\n",
       " '- ворк': 1,\n",
       " 'ворк .': 1,\n",
       " '. а': 1,\n",
       " 'а волк': 1,\n",
       " 'волк -': 1,\n",
       " '- это': 1,\n",
       " 'это ходить': 1,\n",
       " 'ходить .': 1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# можем посмотреть статистики для n-грамм\n",
    "tokens_stat = stat_lm.get_stat()\n",
    "print(tokens_stat.keys())\n",
    "tokens_stat['n+1 tokens stat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae97e9cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:34.658766Z",
     "iopub.status.busy": "2024-02-21T15:03:34.658010Z",
     "iopub.status.idle": "2024-02-21T15:03:34.666999Z",
     "shell.execute_reply": "2024-02-21T15:03:34.665955Z",
     "shell.execute_reply.started": "2024-02-21T15:03:34.658733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вход: Работа\n",
      "Новый токен: не\n"
     ]
    }
   ],
   "source": [
    "print('Вход:', test_text)\n",
    "print('Новый токен:', stat_lm.generate_token(test_text)['next_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b1182bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:37.803765Z",
     "iopub.status.busy": "2024-02-21T15:03:37.803391Z",
     "iopub.status.idle": "2024-02-21T15:03:37.811690Z",
     "shell.execute_reply": "2024-02-21T15:03:37.810737Z",
     "shell.execute_reply.started": "2024-02-21T15:03:37.803740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вход: \"Работа\"\n",
      "Продолженный текст: \"работа не волк . работа не волк . работа не волк . работа не волк . работа не волк . работа не волк . работа не волк . работа не волк .\"\n",
      "Причина остановки генерации: \"max tokens\"\n"
     ]
    }
   ],
   "source": [
    "print(f'Вход: \"{test_text}\"')\n",
    "generated = stat_lm.generate_text(test_text, max_tokens=32)\n",
    "print(f\"Продолженный текст: \\\"{generated['total_text']}\\\"\")\n",
    "print(f\"Причина остановки генерации: \\\"{generated['finish_reason']}\\\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5200218",
   "metadata": {},
   "source": [
    "Как мы видим, модель зациклилась, и начала повторять один и тот же наиболее вероятный кусок текста.\n",
    "Это нередкая проблема для всех генеративных моделей, включая современные LLM.\n",
    "\n",
    "Для борьбы с ней делают две вещи - нормальный трейн-датасет на этапе трейна, и определенные параметры генерации\n",
    "на этапе инференса (например, занизить вероятности в предсказанном распределении у тех токенов, что мы недавно уже генерировали)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a40826e",
   "metadata": {},
   "source": [
    "Также можно использовать другие стратегии семплирования, про которые мы поговорим позднее. Например, sample_top_p в реализации класса позволяет семплировать из наиболее вероятных токенов, вероятности которых суммируются в заданное число p. Можете попробовать эту стратегию, правда, возможно ее нужно будет немного подебажить"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6c0351",
   "metadata": {},
   "source": [
    "### \"Обучим\" LM на n-граммах на полноценном большом датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1320402b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:03:46.715683Z",
     "iopub.status.busy": "2024-02-21T15:03:46.714916Z",
     "iopub.status.idle": "2024-02-21T15:03:51.508767Z",
     "shell.execute_reply": "2024-02-21T15:03:51.507812Z",
     "shell.execute_reply.started": "2024-02-21T15:03:46.715648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.76 s, sys: 1.05 ms, total: 1.76 s\n",
      "Wall time: 1.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = Tokenizer().build_vocab(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33c091d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:04:12.068097Z",
     "iopub.status.busy": "2024-02-21T15:04:12.067691Z",
     "iopub.status.idle": "2024-02-21T15:04:12.074714Z",
     "shell.execute_reply": "2024-02-21T15:04:12.073658Z",
     "shell.execute_reply.started": "2024-02-21T15:04:12.068065Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179367"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d63945",
   "metadata": {},
   "source": [
    "Тут размер словаря посерьезнее"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7febc008",
   "metadata": {},
   "source": [
    "#### >>>> Задание 2: Оцените, насколько возрастет словарь, если убрать один из этапов предобработки - не приводить все тексты к нижнему регистру\n",
    "\n",
    "В качестве ответа используйте отношение размера без предобработка к размеру с предобработкой, округленное как в коде ниже. Например: 15.305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ef31c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.103\n"
     ]
    }
   ],
   "source": [
    "preprocessed_size = len(Tokenizer().build_vocab(all_texts).vocab)\n",
    "without_preprocess_size = len(Tokenizer(lower=False).build_vocab(all_texts).vocab)\n",
    "\n",
    "print( round(without_preprocess_size / preprocessed_size, 3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c70db7f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:05:36.297311Z",
     "iopub.status.busy": "2024-02-21T15:05:36.296909Z",
     "iopub.status.idle": "2024-02-21T15:05:36.404131Z",
     "shell.execute_reply": "2024-02-21T15:05:36.403088Z",
     "shell.execute_reply.started": "2024-02-21T15:05:36.297280Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('tokenizer.pkl', 'wb') as fout:\n",
    "    pickle.dump(tokenizer, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09a9bbda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:12:20.006258Z",
     "iopub.status.busy": "2024-02-21T15:12:20.005532Z",
     "iopub.status.idle": "2024-02-21T15:12:29.164207Z",
     "shell.execute_reply": "2024-02-21T15:12:29.163284Z",
     "shell.execute_reply.started": "2024-02-21T15:12:20.006225Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oplora/Documents/VMK/VK_LLM/nlp/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for IlyaGusev/gazeta contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/IlyaGusev/gazeta\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 5)\n",
      "(500, 5)\n"
     ]
    }
   ],
   "source": [
    "train_texts, test_texts = get_dataset(5000, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cdc5578",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:12:30.368335Z",
     "iopub.status.busy": "2024-02-21T15:12:30.367960Z",
     "iopub.status.idle": "2024-02-21T15:12:42.166853Z",
     "shell.execute_reply": "2024-02-21T15:12:42.165906Z",
     "shell.execute_reply.started": "2024-02-21T15:12:30.368305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3354cf84364a58a06a784890d688a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train lines:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.4 s, sys: 42 ms, total: 4.44 s\n",
      "Wall time: 4.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stat_lm = StatLM(tokenizer, context_size=2, alpha=0.1, sample_top_p = 0.3)\n",
    "stat_lm.train(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27c7ca66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:12:42.169089Z",
     "iopub.status.busy": "2024-02-21T15:12:42.168701Z",
     "iopub.status.idle": "2024-02-21T15:13:46.500417Z",
     "shell.execute_reply": "2024-02-21T15:13:46.499416Z",
     "shell.execute_reply.started": "2024-02-21T15:12:42.169051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "в германии объяснили упоминание имени путина на протестах в берлине . он будет идти на поле . это был подписан в самом деле , что в результате в ходе расследования . в начале второй раз и , в составе . на поле . он уже к этому поводу строительства трассы через два года , что касается матча , в москве . однако уже через две минуты до конца года . по его задержали в начале ноября . однако в списке бомбардиров , что все равно , и в понедельник в следующем году , что на уровне , но на 0 , но это был открыт . но в том , что касается матча , который , а вот и в начале второго периода в то же до конца \n",
      "\n",
      "делегации израиля и сша прибыли в оаэ для обсуждения соглашения о сотрудничестве , в центре москвы , что , а вот так и их было бы не удалось сделать это будет проходить в отличие от общего режима . в том , а также многое другое о том , по словам , и на встрече с требованием удалить спорные вопросы . по словам , в воскресенье в рамках которого будет . и все еще в пятницу в сентябре . в этой страны , в первом тайме хозяева же не удалось . как бы стать крупнейшим в этом году . на странице чемпионата мира . в связи с чем в этот раз . как можно узнать на фоне роста , что , по данным следствия , и я бы на \n",
      "\n",
      "оппозиция белоруссии объявила о создании новой партии вместе с другом . в субботу в москву . я думаю , которые были уверены , как в четверг . по словам , которые уже на то есть проблемы с тех , и они в понедельник в турнирной таблице можно узнать на 10 лет , но это была не в этом в том , в начале сентября , что все же , в первую очередь , что в матче . а не была более того , что это не только , как рассказал газете . в частности , но не будет отвечать на что у нас , а вот и не только в которой можно посмотреть на данный момент , а также были направлены на самом деле . это уже в \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in test_texts[:3]:\n",
    "    title = text.split('\\n')[0]\n",
    "    generated = stat_lm.generate_text(title, max_tokens=128)\n",
    "    print(generated['total_text'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27e04fb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:13:46.502134Z",
     "iopub.status.busy": "2024-02-21T15:13:46.501761Z",
     "iopub.status.idle": "2024-02-21T15:13:59.581842Z",
     "shell.execute_reply": "2024-02-21T15:13:59.580975Z",
     "shell.execute_reply.started": "2024-02-21T15:13:46.502100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c40c3c908f4ea8ab641756e64f9fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train lines:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.4 s, sys: 24 ms, total: 4.43 s\n",
      "Wall time: 4.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stat_lm = StatLM(tokenizer, context_size=2, alpha=0.1, sample_top_p = 0.6)\n",
    "stat_lm.train(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3398c9cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:13:59.584299Z",
     "iopub.status.busy": "2024-02-21T15:13:59.583999Z",
     "iopub.status.idle": "2024-02-21T15:15:05.278370Z",
     "shell.execute_reply": "2024-02-21T15:15:05.277382Z",
     "shell.execute_reply.started": "2024-02-21T15:13:59.584274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "в германии объяснили упоминание имени путина на протестах в берлине . во время назад у кого - 2012 года , сейчас на сайт челябинцев . в москве , что такое решение , 4 , они должны быть право собственности и самый реальный сектор газа в поединке на основании этого преступления , от тоттенхэма . до 500 млн , по подозрению в более 2 , отмечает , но зато совершенно логично , который будет происходить за пределов штрафной гостей . после того , который как рассказал газете . теперь их на поле принимал участие в чем в итоге спокойно . как один человек . когда она . ну а вот я не с . я не менее , сказал источник в возрасте от друга в отношении остальных участников редакционной \n",
      "\n",
      "делегации израиля и сша прибыли в оаэ для обсуждения соглашения о сотрудничестве с питерским клубом до конца встречи : по вопросам международной безопасности и муниципальных чиновников , в нашем производстве и наш голкипер сборной россии . так и правительством сша в столице и расходов , что нас будут назначены . не сумел отправить шайбу в том , все дела , когда у администрации химок была обнаружена . но и испания . потом будут интересны и вырвать ничью . мы не работали . ru в последние два года , чтобы принять решение и 2008 года на работу в этом районе 10 минут , что в 2009 года , как показывает , чтобы остановить машину и требовал передать в сети в рамках реализации большинства граждан , однако , что , \n",
      "\n",
      "оппозиция белоруссии объявила о создании новой партии вместе с , что нужно , но как у одного из - то , в пятницу в первом сете . как , если бы использовать средства , отметил в россии . напомним , а также отметил , так , который позволяет . но для самих хоккеистов , что же ключе , в основном составе команды . а также идут по всем участникам . в столице будет окончательно отступил от двух матчей и алексей волков динамо в прошлом году , а также , в пятницу ночью в третьем месте работы , и андерсон варежао . но в атаке гости открыли счет , после этого не сможет даже в первом матче . впрочем , которые у нас не будет . тот же категории \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in test_texts[:3]:\n",
    "    title = text.split('\\n')[0]\n",
    "    generated = stat_lm.generate_text(title, max_tokens=128)\n",
    "    print(generated['total_text'], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d13da3d",
   "metadata": {},
   "source": [
    "Попробуйте перебрать параметры в поисках лучшего качества модели: context_size, alpha, и стратегию генерации sample_top_p=0.9 (возможно, эту стратегию придется немного подебажить)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9706e",
   "metadata": {},
   "source": [
    "## LM на основе RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3546682",
   "metadata": {},
   "source": [
    "Рекуррентные сети формально могут обрабатывать последовательность произвольной длины - \n",
    "можем просто через RNN-ячейку рекуррентно пропускать последовательность, а в конце посчитать градиент\n",
    "по этой большой цепочке.\n",
    "\n",
    "На практике все равно есть ограничения:\n",
    "1. Градиент по большой последовательности очень долго считать - с помощью backpropagation through time (BPTT) для каждого элемента последовательности придется считать градиент\n",
    "2. Для ускорения обучения и для лучшей сходимости тренировочные примеры (то есть, последовательности = тексты) объединяют во время обучения в батчи. Чтобы батч представить в виде тензора, все последовательности нужно выравнить (мы же не можем в матрице сделать длины строк разными). Для этого используются последовательности с фиксированной длиной, называемой длиной контекста."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4049a583",
   "metadata": {},
   "source": [
    "Теперь давайте построим LM с помощью рекуррентной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eee1438",
   "metadata": {},
   "source": [
    "Попробуем использовать tokenizer из прошлого раздела, уже обученный на словах "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e448ea0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:15:22.202036Z",
     "iopub.status.busy": "2024-02-21T15:15:22.201672Z",
     "iopub.status.idle": "2024-02-21T15:15:22.317216Z",
     "shell.execute_reply": "2024-02-21T15:15:22.316365Z",
     "shell.execute_reply.started": "2024-02-21T15:15:22.202010Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('tokenizer.pkl', 'rb') as fin:\n",
    "    tokenizer = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0dd288fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:15:22.989880Z",
     "iopub.status.busy": "2024-02-21T15:15:22.989475Z",
     "iopub.status.idle": "2024-02-21T15:15:22.996299Z",
     "shell.execute_reply": "2024-02-21T15:15:22.995312Z",
     "shell.execute_reply.started": "2024-02-21T15:15:22.989850Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179367"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdf94f4",
   "metadata": {},
   "source": [
    "Составим датасеты в torch-формате"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c5c4a85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:15:25.086845Z",
     "iopub.status.busy": "2024-02-21T15:15:25.086448Z",
     "iopub.status.idle": "2024-02-21T15:15:25.095630Z",
     "shell.execute_reply": "2024-02-21T15:15:25.094722Z",
     "shell.execute_reply.started": "2024-02-21T15:15:25.086815Z"
    }
   },
   "outputs": [],
   "source": [
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, inputs: List[List[int]], targets: Optional[List[List[int]]] = None):\n",
    "        self.inputs = torch.LongTensor(inputs)\n",
    "        self.targets = None\n",
    "        if targets is not None:\n",
    "            self.targets = torch.LongTensor(targets)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int) -> (List[str], int):\n",
    "        if self.targets is None:\n",
    "            return self.inputs[idx]\n",
    "        else:\n",
    "            return self.inputs[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19a4a76e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:15:26.881701Z",
     "iopub.status.busy": "2024-02-21T15:15:26.881179Z",
     "iopub.status.idle": "2024-02-21T15:15:26.888798Z",
     "shell.execute_reply": "2024-02-21T15:15:26.887738Z",
     "shell.execute_reply.started": "2024-02-21T15:15:26.881667Z"
    }
   },
   "outputs": [],
   "source": [
    "context_size = 32\n",
    "\n",
    "def get_tokenized_data(tokenizer: Tokenizer, \n",
    "                       texts: List[str], \n",
    "                       context_size: int) -> (List[List[int]], List[List[int]]):\n",
    "    tokenized_inputs, tokenized_targets = [], []\n",
    "    for text in tqdm(texts):\n",
    "        tokens = tokenizer.encode(text, append_eos_token=True)\n",
    "        for i in range(len(tokens) - context_size):\n",
    "            inputs = tokens[i: i + context_size]\n",
    "            targets = tokens[i + 1: i + 1 + context_size]\n",
    "            tokenized_inputs.append(inputs)\n",
    "            tokenized_targets.append(targets)\n",
    "    return tokenized_inputs, tokenized_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4174279a",
   "metadata": {},
   "source": [
    "Возьмем для начала малое количество текстов, просто чтобы отладить процесс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7dff249c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:15:27.709763Z",
     "iopub.status.busy": "2024-02-21T15:15:27.708412Z",
     "iopub.status.idle": "2024-02-21T15:15:27.717477Z",
     "shell.execute_reply": "2024-02-21T15:15:27.716395Z",
     "shell.execute_reply.started": "2024-02-21T15:15:27.709716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 500)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts), len(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2143a592",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:15:29.886137Z",
     "iopub.status.busy": "2024-02-21T15:15:29.885722Z",
     "iopub.status.idle": "2024-02-21T15:15:32.388945Z",
     "shell.execute_reply": "2024-02-21T15:15:32.387996Z",
     "shell.execute_reply.started": "2024-02-21T15:15:29.886107Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39432c106aa4daca0a0801e886b0f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4b5cd6affd410a8b438b7a2722137e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 32]), torch.Size([32, 32]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "tokenized_inputs_train, tokenized_targets_train = get_tokenized_data(tokenizer, train_texts[:120], context_size)\n",
    "tokenized_inputs_test, tokenized_targets_test = get_tokenized_data(tokenizer, test_texts[:60], context_size)\n",
    "\n",
    "train_dataset = NewsDataset(tokenized_inputs_train, tokenized_targets_train)\n",
    "test_dataset = NewsDataset(tokenized_inputs_test, tokenized_targets_test)\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "t0, t1 = next(iter(train_dl))\n",
    "t0.shape, t1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407bd892",
   "metadata": {},
   "source": [
    "Параметры обучения и модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57eb8842",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:15:33.474688Z",
     "iopub.status.busy": "2024-02-21T15:15:33.474200Z",
     "iopub.status.idle": "2024-02-21T15:15:33.530087Z",
     "shell.execute_reply": "2024-02-21T15:15:33.528769Z",
     "shell.execute_reply.started": "2024-02-21T15:15:33.474657Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer_params = {}\n",
    "\n",
    "model_params = {\n",
    "    'vocab_size': len(tokenizer.vocab),\n",
    "    'embed_dim': 300,\n",
    "    'hidden_size': 64\n",
    "}\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a01275",
   "metadata": {},
   "source": [
    "Архитектура сети представлена ниже. \n",
    "\n",
    "Здесь мы каждому токену из последовательности сопоставляем обучаемый вектор с помощью слоя nn.Embedding, и всего таких векторов слой знает vocab_size штук, где vocab_size - размер словаря.\n",
    "\n",
    "Каждый такой обучаемый вектор имеет размерность embed_dim.\n",
    "\n",
    "После того, как мы получим \"умное\" представление всей последовательности - вектор размерности hidden_size, мы хотим отобразить его в пространство словаря, и это отображение (вектор) мы будем использовать как логиты, из которых получается вероятностное распределение для следующего токена. За это отображение отвечает линейный слой.\n",
    "\n",
    "Заполните параметры nn.Embedding и nn.Linear с учетом написанного выше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd53d807",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:15:35.573105Z",
     "iopub.status.busy": "2024-02-21T15:15:35.572397Z",
     "iopub.status.idle": "2024-02-21T15:15:35.582184Z",
     "shell.execute_reply": "2024-02-21T15:15:35.581210Z",
     "shell.execute_reply.started": "2024-02-21T15:15:35.573069Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class RecLM(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_size: int):\n",
    "        super(RecLM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim) # TODO заполнить размерности\n",
    "        self.rnn = nn.RNNCell(embed_dim, hidden_size, nonlinearity='tanh', bias=True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)  # TODO заполнить размерности\n",
    "        # иногда вместо этого используют ту же матрицу, что в nn.Embedding \n",
    "        \n",
    "    def forward(self, inputs: torch.LongTensor):\n",
    "        \"\"\"\n",
    "        inputs: int, bs x seq\n",
    "        # pad_mask: bool, bs x seq\n",
    "        \"\"\"\n",
    "        embed = self.embed(inputs) # bs x seq x dim\n",
    "        h_n = None\n",
    "        outputs = []\n",
    "        for seq_elem in embed.transpose(0, 1):\n",
    "            if h_n is None:\n",
    "                h_n = self.rnn(seq_elem) # bs x out_dim\n",
    "            else:\n",
    "                h_n = self.rnn(seq_elem, h_n)\n",
    "            outputs.append(h_n)\n",
    "        \n",
    "        outputs = torch.stack(outputs).transpose(0, 1) # bs x seq x dim\n",
    "        return self.linear(outputs) # bs x seq x vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6dfe9e37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:15:37.180182Z",
     "iopub.status.busy": "2024-02-21T15:15:37.179555Z",
     "iopub.status.idle": "2024-02-21T15:15:37.832692Z",
     "shell.execute_reply": "2024-02-21T15:15:37.831683Z",
     "shell.execute_reply.started": "2024-02-21T15:15:37.180153Z"
    }
   },
   "outputs": [],
   "source": [
    "model = RecLM(vocab_size=model_params['vocab_size'], \n",
    "              embed_dim=model_params['embed_dim'], \n",
    "              hidden_size=model_params['hidden_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302cff51",
   "metadata": {},
   "source": [
    "В качестве лосса используем кросс-энтропию - ведь по сути мы решаем задачу многоклассовой классификации, где классы - это токены из словаря. Почитать про лосс можно ниже:\n",
    "\n",
    "nn.CrossEntropyLoss: https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a1c098",
   "metadata": {},
   "source": [
    "Напишем класс Trainer , с помощью которого мы будем обучать нейронку. Изучите, что происходит в коде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f3096a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:42:30.945295Z",
     "iopub.status.busy": "2024-02-21T15:42:30.944240Z",
     "iopub.status.idle": "2024-02-21T15:42:30.961191Z",
     "shell.execute_reply": "2024-02-21T15:42:30.960169Z",
     "shell.execute_reply.started": "2024-02-21T15:42:30.945257Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model_params: dict,\n",
    "        optimizer_params: dict, \n",
    "        n_epochs: int,\n",
    "        train_dataloader: torch.utils.data.DataLoader, \n",
    "        val_dataloader: torch.utils.data.DataLoader,\n",
    "        device: str\n",
    "    ):\n",
    "\n",
    "        self.model = RecLM(**model_params)\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(self.device)\n",
    "        self.optimizer = torch.optim.AdamW(params=self.model.parameters(), **optimizer_params)\n",
    "        \n",
    "        ts = datetime.strftime(datetime.today(), '%Y-%m-%d-%H-%M')\n",
    "        LOG_DIR = f'./checkpoints/{ts}'\n",
    "        \n",
    "        hparams = chain(model_params.items(), optimizer_params.items())\n",
    "\n",
    "        for k, v in hparams:\n",
    "            LOG_DIR += f'-{k}-{v}'\n",
    "        \n",
    "        self.writer = SummaryWriter(log_dir=LOG_DIR)\n",
    "        self.checkpoint_path = LOG_DIR\n",
    "\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.n_epochs = n_epochs\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        \n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss(reduction = 'none')\n",
    "\n",
    "    def iterate_over_dataloader(self, dataloader, suffix: str, epoch: int, update_weights=False):\n",
    "        \n",
    "        if update_weights:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "        \n",
    "        loss_over_epoch = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch in tqdm(dataloader, desc='batches'):\n",
    "\n",
    "            if update_weights:\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            tokens, labels = batch\n",
    "            \n",
    "            tokens = tokens.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            logits = self.model(tokens) # bs x seq x vocab_size\n",
    "            \n",
    "            loss_values = self.loss_fn(logits.transpose(1, 2), labels) # N x C x seq_len\n",
    "            loss_value = loss_values.mean()\n",
    "            \n",
    "            if update_weights:\n",
    "                loss_value.backward()\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            loss_item = loss_value.item()\n",
    "            loss_over_epoch += loss_item\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = loss_over_epoch / num_batches\n",
    "        print(f'Epoch {epoch} loss for {suffix}: {avg_loss}')\n",
    "        self.writer.add_scalar(tag=f'Loss/{suffix}', scalar_value=avg_loss, global_step=epoch)\n",
    "\n",
    "        if avg_loss < self.best_val_loss:\n",
    "            self.best_val_loss = loss_item\n",
    "            torch.save(self.model.state_dict(), os.path.join(self.checkpoint_path, 'rnn_lm.pt'))\n",
    "\n",
    "            \n",
    "    def train_model(self):\n",
    "        for epoch_num in tqdm(range(self.n_epochs)):\n",
    "            \n",
    "            self.iterate_over_dataloader(\n",
    "                dataloader=self.train_dataloader, suffix='train', epoch=epoch_num, update_weights=True\n",
    "            )\n",
    "            self.iterate_over_dataloader(\n",
    "                dataloader=self.val_dataloader, suffix='val', epoch=epoch_num\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386edd2e",
   "metadata": {},
   "source": [
    "И прежде чем обучать, давайте посмотрим, насколько большая у нас получилась модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b06b19e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:15:39.766891Z",
     "iopub.status.busy": "2024-02-21T15:15:39.766500Z",
     "iopub.status.idle": "2024-02-21T15:15:39.774353Z",
     "shell.execute_reply": "2024-02-21T15:15:39.773432Z",
     "shell.execute_reply.started": "2024-02-21T15:15:39.766866Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_nn_params_stat(model: nn.Module) -> None:\n",
    "    \n",
    "    def iter_mul(inputs: Iterable) -> int:\n",
    "        mul = 1\n",
    "        for elem in inputs:\n",
    "            mul *= elem\n",
    "        return mul\n",
    "    \n",
    "    shapes = [p.shape for p in model.parameters()]\n",
    "    for p_shape in shapes:\n",
    "        print(p_shape)\n",
    "    total_count = sum([iter_mul(p_shape) for p_shape in shapes])\n",
    "    print('Total params:', total_count)\n",
    "    print('Model param size in Mb:', total_count * 4 / (2 ** 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f0973c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:15:43.163192Z",
     "iopub.status.busy": "2024-02-21T15:15:43.162310Z",
     "iopub.status.idle": "2024-02-21T15:15:43.168020Z",
     "shell.execute_reply": "2024-02-21T15:15:43.167092Z",
     "shell.execute_reply.started": "2024-02-21T15:15:43.163156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([179367, 300])\n",
      "torch.Size([64, 300])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([179367, 64])\n",
      "torch.Size([179367])\n",
      "Total params: 65492379\n",
      "Model param size in Mb: 249.83359909057617\n"
     ]
    }
   ],
   "source": [
    "get_nn_params_stat(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbcb030",
   "metadata": {},
   "source": [
    "В гугл колабе такая модель на гпу не поместится, а если бы и поместилась - обучать такое пришлось бы долго.\n",
    "\n",
    "Как мы видим, подавляющая часть параметров содержится в тензорах torch.Size([179367, 300]) и torch.Size([179367, 64]) - это параметры слоя эмбеддинга, при этом 179367 - это размерность словаря\n",
    "\n",
    "И, как правило, во всех подобных сетях слой nn.Embedding содержит много параметров, и в разных статьях этот слой пытаются как-то \"облегчить\" - либо декомпозировать, либо уменьшить размер словаря. \n",
    "\n",
    "Мы пойдем по пути уменьшения словаря, но для этого нам нужно представить текст не словами, а более оптимальными языковыми единицами. Это достигается с помощью BPE кодирования, которое мы будем обсуджать в курсе. А пока, давайте просто им воспользуемся, после небольшого задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "407fe872",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:18:41.782962Z",
     "iopub.status.busy": "2024-02-21T15:18:41.782575Z",
     "iopub.status.idle": "2024-02-21T15:18:42.120143Z",
     "shell.execute_reply": "2024-02-21T15:18:42.118947Z",
     "shell.execute_reply.started": "2024-02-21T15:18:41.782932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([89683, 300])\n",
      "torch.Size([64, 300])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([89683, 64])\n",
      "torch.Size([89683])\n",
      "Total params: 32757719\n",
      "Model param size in Mb: 124.96078109741211\n"
     ]
    }
   ],
   "source": [
    "model = RecLM(vocab_size=model_params['vocab_size'] // 2, \n",
    "              embed_dim=model_params['embed_dim'], \n",
    "              hidden_size=model_params['hidden_size'])\n",
    "\n",
    "get_nn_params_stat(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121d23dd",
   "metadata": {},
   "source": [
    "#### >>>> Задание 3\n",
    "\n",
    "посмотрите, во сколько раз уменьшиться размер модели (в мегабайтах), если уменьшить в два раза размер словаря.  Пример: 5.423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d4b58936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.999"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round( 249.83359909057617 / 124.96078109741211, 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92537323",
   "metadata": {},
   "source": [
    "#### >>>> Задание 4\n",
    "\n",
    "Допустим, мы знаем, что на нашей гпу-карте мы можем выделить только 15 мб для нашей модели (при этом, все остальные ресурсы под оптимизатор, датасет и прочее уже учтены). В какое минимальное целое кол-во раз нужно уменьшить размер словаря, чтобы выполнить это требование?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "acfa3c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10551, 300])\n",
      "torch.Size([64, 300])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([10551, 64])\n",
      "torch.Size([10551])\n",
      "Total params: 3874539\n",
      "Model param size in Mb: 14.780193328857422\n",
      "Ответ: 17\n"
     ]
    }
   ],
   "source": [
    "max_drop = 17\n",
    "\n",
    "model = RecLM(vocab_size=model_params['vocab_size'] // max_drop,\n",
    "              embed_dim=model_params['embed_dim'], \n",
    "              hidden_size=model_params['hidden_size'])\n",
    "\n",
    "get_nn_params_stat(model)\n",
    "print(f'Ответ: {max_drop}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e67ef8",
   "metadata": {},
   "source": [
    "Ответ - целое число, например 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73cda48",
   "metadata": {},
   "source": [
    "### Уменьшим словарь\n",
    "\n",
    "Обучим BPE-токенизатор на нашем корпусе. Суть несложная - в качестве токенов мы будем использовать последовательности символов, которые оптимальнее всего сжимают текст (если последовательность токенов встречается очень часто - будем использовать ее как токен, и так итеративно можем построить словарь фиксированного размера). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "84367be1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:21:52.214328Z",
     "iopub.status.busy": "2024-02-21T15:21:52.213923Z",
     "iopub.status.idle": "2024-02-21T15:21:52.219645Z",
     "shell.execute_reply": "2024-02-21T15:21:52.218528Z",
     "shell.execute_reply.started": "2024-02-21T15:21:52.214298Z"
    }
   },
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5d74f6de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:21:53.601698Z",
     "iopub.status.busy": "2024-02-21T15:21:53.600936Z",
     "iopub.status.idle": "2024-02-21T15:21:53.605988Z",
     "shell.execute_reply": "2024-02-21T15:21:53.604938Z",
     "shell.execute_reply.started": "2024-02-21T15:21:53.601666Z"
    }
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1966b254",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:21:53.818275Z",
     "iopub.status.busy": "2024-02-21T15:21:53.817491Z",
     "iopub.status.idle": "2024-02-21T15:21:53.829598Z",
     "shell.execute_reply": "2024-02-21T15:21:53.828457Z",
     "shell.execute_reply.started": "2024-02-21T15:21:53.818245Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "074203ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:21:54.208604Z",
     "iopub.status.busy": "2024-02-21T15:21:54.208223Z",
     "iopub.status.idle": "2024-02-21T15:21:54.213383Z",
     "shell.execute_reply": "2024-02-21T15:21:54.212380Z",
     "shell.execute_reply.started": "2024-02-21T15:21:54.208573Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[BOS] $A [EOS]\",\n",
    "    special_tokens=[(\"[BOS]\", 1), (\"[EOS]\", 4)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a678c092",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:21:55.873748Z",
     "iopub.status.busy": "2024-02-21T15:21:55.873376Z",
     "iopub.status.idle": "2024-02-21T15:21:55.878995Z",
     "shell.execute_reply": "2024-02-21T15:21:55.877931Z",
     "shell.execute_reply.started": "2024-02-21T15:21:55.873722Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = BpeTrainer(vocab_size=VOCAB_SIZE, \n",
    "                     special_tokens=[\"[UNK]\", \"[BOS]\", \"[SEP]\", \"[PAD]\", \"[EOS]\"],\n",
    "                    show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ce821f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:21:56.222033Z",
     "iopub.status.busy": "2024-02-21T15:21:56.221651Z",
     "iopub.status.idle": "2024-02-21T15:21:56.227796Z",
     "shell.execute_reply": "2024-02-21T15:21:56.226801Z",
     "shell.execute_reply.started": "2024-02-21T15:21:56.222003Z"
    }
   },
   "outputs": [],
   "source": [
    "def simple_preprocess(text: str) -> str:\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5f40e248",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:21:56.417784Z",
     "iopub.status.busy": "2024-02-21T15:21:56.417478Z",
     "iopub.status.idle": "2024-02-21T15:21:56.424025Z",
     "shell.execute_reply": "2024-02-21T15:21:56.423008Z",
     "shell.execute_reply.started": "2024-02-21T15:21:56.417758Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 500)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts), len(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "12a936af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:21:58.321546Z",
     "iopub.status.busy": "2024-02-21T15:21:58.321141Z",
     "iopub.status.idle": "2024-02-21T15:21:58.620086Z",
     "shell.execute_reply": "2024-02-21T15:21:58.619119Z",
     "shell.execute_reply.started": "2024-02-21T15:21:58.321515Z"
    }
   },
   "outputs": [],
   "source": [
    "train_texts = list(map(simple_preprocess, train_texts))\n",
    "test_texts = list(map(simple_preprocess, test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d889bc1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:21:58.840914Z",
     "iopub.status.busy": "2024-02-21T15:21:58.840541Z",
     "iopub.status.idle": "2024-02-21T15:21:59.030530Z",
     "shell.execute_reply": "2024-02-21T15:21:59.029454Z",
     "shell.execute_reply.started": "2024-02-21T15:21:58.840885Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('new_train.txt', 'w') as fout:\n",
    "    print('\\n'.join(train_texts), file=fout)\n",
    "    \n",
    "with open('new_test.txt', 'w') as fout:\n",
    "    print('\\n'.join(test_texts), file=fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f6c7e93b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:22:00.157573Z",
     "iopub.status.busy": "2024-02-21T15:22:00.157158Z",
     "iopub.status.idle": "2024-02-21T15:22:05.272325Z",
     "shell.execute_reply": "2024-02-21T15:22:05.271363Z",
     "shell.execute_reply.started": "2024-02-21T15:22:00.157540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CPU times: user 11.7 s, sys: 1.24 s, total: 12.9 s\n",
      "Wall time: 2.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "files = [f'new_{key}.txt' for key in ['train', 'test']]\n",
    "tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4e5193e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:22:05.274413Z",
     "iopub.status.busy": "2024-02-21T15:22:05.274092Z",
     "iopub.status.idle": "2024-02-21T15:22:05.281264Z",
     "shell.execute_reply": "2024-02-21T15:22:05.280263Z",
     "shell.execute_reply.started": "2024-02-21T15:22:05.274387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d7750fae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:22:05.282709Z",
     "iopub.status.busy": "2024-02-21T15:22:05.282261Z",
     "iopub.status.idle": "2024-02-21T15:22:05.292682Z",
     "shell.execute_reply": "2024-02-21T15:22:05.291735Z",
     "shell.execute_reply.started": "2024-02-21T15:22:05.282667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[BOS]', '[UNK]', 'от', 'такие', 'пи', 'ро', 'ги', '!', '[EOS]']\n",
      "['[BOS]', 'вот', 'такие', 'пи', 'ро', 'ги', '!', '[EOS]']\n",
      "[1, 1119, 2033, 236, 146, 234, 5, 4]\n",
      "[1, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "text = 'Вот такие пироги!'\n",
    "\n",
    "print(tokenizer.encode(text).tokens)\n",
    "print(tokenizer.encode(text.lower()).tokens)\n",
    "print(tokenizer.encode(text.lower()).ids)\n",
    "print(tokenizer.encode(text.lower()).special_tokens_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0ab4ecbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:22:35.024361Z",
     "iopub.status.busy": "2024-02-21T15:22:35.023692Z",
     "iopub.status.idle": "2024-02-21T15:22:35.035639Z",
     "shell.execute_reply": "2024-02-21T15:22:35.034667Z",
     "shell.execute_reply.started": "2024-02-21T15:22:35.024316Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[UNK]'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{ind: token for token, ind in tokenizer.get_vocab().items()}[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d2545884",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:22:36.072989Z",
     "iopub.status.busy": "2024-02-21T15:22:36.072093Z",
     "iopub.status.idle": "2024-02-21T15:22:36.082186Z",
     "shell.execute_reply": "2024-02-21T15:22:36.081125Z",
     "shell.execute_reply.started": "2024-02-21T15:22:36.072952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()['[EOS]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bdb121",
   "metadata": {},
   "source": [
    "Токенизируем трейн и тест данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9188a68b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:22:39.354702Z",
     "iopub.status.busy": "2024-02-21T15:22:39.354330Z",
     "iopub.status.idle": "2024-02-21T15:22:39.361746Z",
     "shell.execute_reply": "2024-02-21T15:22:39.360787Z",
     "shell.execute_reply.started": "2024-02-21T15:22:39.354675Z"
    }
   },
   "outputs": [],
   "source": [
    "context_size = 32\n",
    "\n",
    "def get_bpe_tokenized_data(tokenizer: tokenizers.Tokenizer, \n",
    "                           texts: List[str], \n",
    "                           context_size: int) -> (List[List[int]], List[List[int]]):\n",
    "    tokenized_inputs, tokenized_targets = [], []\n",
    "    for text in tqdm(texts):\n",
    "        tokens = tokenizer.encode(text).ids\n",
    "        for i in range(len(tokens) - context_size):\n",
    "            inputs = tokens[i: i + context_size]\n",
    "            targets = tokens[i + 1: i + 1 + context_size]\n",
    "            tokenized_inputs.append(inputs)\n",
    "            tokenized_targets.append(targets)\n",
    "    return tokenized_inputs, tokenized_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f699366b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:22:52.476025Z",
     "iopub.status.busy": "2024-02-21T15:22:52.475234Z",
     "iopub.status.idle": "2024-02-21T15:23:13.168200Z",
     "shell.execute_reply": "2024-02-21T15:23:13.167254Z",
     "shell.execute_reply.started": "2024-02-21T15:22:52.475991Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f730d2b6e645b1a367ce487daa69f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94abdf943f7a4d0a89852556e6a51bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 32]), torch.Size([32, 32]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "tokenized_inputs_train, tokenized_targets_train = get_bpe_tokenized_data(tokenizer, train_texts[:1024], context_size)\n",
    "tokenized_inputs_test, tokenized_targets_test = get_bpe_tokenized_data(tokenizer, test_texts[:128], context_size)\n",
    "\n",
    "train_dataset = NewsDataset(tokenized_inputs_train, tokenized_targets_train)\n",
    "test_dataset = NewsDataset(tokenized_inputs_test, tokenized_targets_test)\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "t0, t1 = next(iter(train_dl))\n",
    "t0.shape, t1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3105169b",
   "metadata": {},
   "source": [
    "Чтобы эксперименты можно запустить на Kaggle или Google Colab, выставим небольшие параметры (размер словаря выбран по тем же принципам быстроты экспериментов, но обычно он выбирается 30_000 или в радиусе). \n",
    "\n",
    "Если у вас есть собственные мощности либо свободное время, можете дополнительно поэкспермиентировать с сетями помощнее и перебрать параметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5c2bce56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:23:14.239621Z",
     "iopub.status.busy": "2024-02-21T15:23:14.238696Z",
     "iopub.status.idle": "2024-02-21T15:23:14.246709Z",
     "shell.execute_reply": "2024-02-21T15:23:14.245656Z",
     "shell.execute_reply.started": "2024-02-21T15:23:14.239571Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer_params = {}\n",
    "\n",
    "model_params = {\n",
    "    'vocab_size': tokenizer.get_vocab_size(),\n",
    "    'embed_dim': 300,\n",
    "    'hidden_size': 64\n",
    "}\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "n_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c7a2796d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:23:18.014473Z",
     "iopub.status.busy": "2024-02-21T15:23:18.013601Z",
     "iopub.status.idle": "2024-02-21T15:23:18.036092Z",
     "shell.execute_reply": "2024-02-21T15:23:18.035366Z",
     "shell.execute_reply.started": "2024-02-21T15:23:18.014429Z"
    }
   },
   "outputs": [],
   "source": [
    "model = RecLM(vocab_size=model_params['vocab_size'], \n",
    "              embed_dim=model_params['embed_dim'], \n",
    "              hidden_size=model_params['hidden_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9900d7cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:23:18.539714Z",
     "iopub.status.busy": "2024-02-21T15:23:18.538920Z",
     "iopub.status.idle": "2024-02-21T15:23:18.545627Z",
     "shell.execute_reply": "2024-02-21T15:23:18.544165Z",
     "shell.execute_reply.started": "2024-02-21T15:23:18.539671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 300])\n",
      "torch.Size([64, 300])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([5000, 64])\n",
      "torch.Size([5000])\n",
      "Total params: 1848424\n",
      "Model param size in Mb: 7.051177978515625\n"
     ]
    }
   ],
   "source": [
    "get_nn_params_stat(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1630ee0",
   "metadata": {},
   "source": [
    "Можете оценить, во сколько раз изменился размер сети, при этом весь текст мы кодируем, и никакие токены не теряем (в случае токенизации по словам, выкидывание слова означало потерю сигнала и замену токена на Unknown)\n",
    "\n",
    "Вывод: большую часть параметров подобных сетей содержит слой эмбеддинга. Во многих работах это пытаются исправить: декомпозировать этот слой низкоранговыми матрицами, переиспользовать слой эмбеддингов из начала сети в конце и др. Ну и самое главное - использовать словарь поменьше, 30_000 - классический размер словаря с BPE-токенизацией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6657ea21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:42:48.873448Z",
     "iopub.status.busy": "2024-02-21T15:42:48.873043Z",
     "iopub.status.idle": "2024-02-21T15:42:48.899231Z",
     "shell.execute_reply": "2024-02-21T15:42:48.898334Z",
     "shell.execute_reply.started": "2024-02-21T15:42:48.873416Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model_params, optimizer_params, n_epochs,\n",
    "       train_dl, test_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7b49b5aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:42:53.537970Z",
     "iopub.status.busy": "2024-02-21T15:42:53.537602Z",
     "iopub.status.idle": "2024-02-21T15:42:53.542442Z",
     "shell.execute_reply": "2024-02-21T15:42:53.541410Z",
     "shell.execute_reply.started": "2024-02-21T15:42:53.537943Z"
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90105874",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:42:54.699303Z",
     "iopub.status.busy": "2024-02-21T15:42:54.698927Z",
     "iopub.status.idle": "2024-02-21T15:49:49.150697Z",
     "shell.execute_reply": "2024-02-21T15:49:49.149732Z",
     "shell.execute_reply.started": "2024-02-21T15:42:54.699273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1bfcb2d648c4fa1be8118e61136d0bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfcb35a3a79449da36e703f66629ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "batches:   0%|          | 0/34677 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45fdfff",
   "metadata": {},
   "source": [
    "## Результат\n",
    "\n",
    "С помощью обученной модели можно что-нибудь сгенерировать с помощью кода ниже. Поисследуйте поведение модели, насколько разнообразный текс генерируется, насколько текст правдоподбный"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78734406",
   "metadata": {},
   "source": [
    "#### >>>> Задание 5.\n",
    "\n",
    "В лекциях мы рассматривали, как получить из вектора логитов вероятностное распределение (все числа в векторе суммируются в 1, все числа от 0 до 1). Эта функция также реализована в torch, в модуле torch.nn.functional. Допишите в методе _ get_next_token только эту функцию (при ее добавлении код в ячейках ниже не должен падать), и это же слово используйте в качестве ответа. Например: relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce442748",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:49:49.153702Z",
     "iopub.status.busy": "2024-02-21T15:49:49.153229Z",
     "iopub.status.idle": "2024-02-21T15:49:49.174432Z",
     "shell.execute_reply": "2024-02-21T15:49:49.173408Z",
     "shell.execute_reply.started": "2024-02-21T15:49:49.153659Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextGenerator:\n",
    "    def __init__(self, \n",
    "                 model: nn.Module, \n",
    "                 tokenizer: tokenizers.Tokenizer,\n",
    "                 context_size: int,\n",
    "                 eos_token_id: int,\n",
    "                 sample_top_p: Optional[float] = None):\n",
    "        \n",
    "        self.model = model.eval()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.context_size = context_size\n",
    "        self.sample_top_p = sample_top_p\n",
    "        \n",
    "    def sample_token(self, token_distribution: np.ndarray) -> int:\n",
    "        if self.sample_top_p is None:\n",
    "            return token_distribution.argmax()\n",
    "        else:\n",
    "            token_distribution = sorted(list(zip(token_distribution, np.arange(len(token_distribution)))))\n",
    "            total_proba = 0.0\n",
    "            tokens_to_sample = []\n",
    "            tokens_probas = []\n",
    "            for token_proba, ind in sorted(token_distribution, reverse=True):\n",
    "                tokens_to_sample.append(ind)\n",
    "                tokens_probas.append(token_proba)\n",
    "                total_proba += token_proba\n",
    "                if total_proba >= self.sample_top_p:\n",
    "                    break\n",
    "            # для простоты отнормируем вероятности, чтобы суммировались в единицу\n",
    "            tokens_probas = np.array(tokens_probas)\n",
    "            tokens_probas = tokens_probas / tokens_probas.sum()\n",
    "            return np.random.choice(tokens_to_sample, p=tokens_probas)\n",
    "    \n",
    "    \n",
    "    def _get_next_token(self, tokens: List[int]) -> (int, str):\n",
    "        tensor_inputs = torch.LongTensor([tokens])\n",
    "        with torch.no_grad():\n",
    "            logits = model(tensor_inputs)[0][-1]\n",
    "            func = torch.nn.functional. ... # TODO используем функцию из torch.nn.functional, чтобы получить вероятности\n",
    "            token_distribution = func(logits)\n",
    "            max_proba_ind = self.sample_token(token_distribution.numpy())\n",
    "        \n",
    "        # print(token_distribution.shape, token_distribution)\n",
    "        next_token = self.tokenizer.id_to_token(max_proba_ind)\n",
    "        \n",
    "        return max_proba_ind, next_token\n",
    "            \n",
    "    def generate_token(self, text: str, remove_special_tokens: bool = False) -> Dict:\n",
    "        tokens = self.tokenizer.encode(text.lower()).ids\n",
    "        if tokens[-1] == self.eos_token_id:\n",
    "            tokens.pop()\n",
    "        tokens = tokens[-self.context_size:]\n",
    "        max_proba_ind, next_token = self._get_next_token(tokens)\n",
    "        \n",
    "        return {\n",
    "            'next_token': next_token,\n",
    "            'next_token_num': max_proba_ind,\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def generate_text(self, \n",
    "                      text: str,\n",
    "                      max_tokens: int, \n",
    "                      remove_special_tokens: bool = False,\n",
    "                      ) -> Dict:\n",
    "        \n",
    "        all_tokens = tokenizer.encode(text.lower()).ids\n",
    "        if all_tokens[-1] == self.eos_token_id:\n",
    "            all_tokens.pop()\n",
    "        tokens = all_tokens[-self.context_size:]\n",
    "        if not tokens:\n",
    "            return {\n",
    "                'all_tokens': all_tokens,\n",
    "                'total_text': '',\n",
    "                'finish_reason': 'empty_input'\n",
    "            }\n",
    "        \n",
    "        max_proba_ind = None\n",
    "        while max_proba_ind != self.eos_token_id and len(all_tokens) < max_tokens:\n",
    "            max_proba_ind, next_token = self._get_next_token(tokens)\n",
    "            all_tokens.append(max_proba_ind)\n",
    "            tokens = all_tokens[-self.context_size:]\n",
    "        \n",
    "        new_text = self.tokenizer.decode(all_tokens, remove_special_tokens)\n",
    "        \n",
    "        finish_reason = 'max tokens'\n",
    "        if all_tokens[-1] == self.eos_token_id:\n",
    "            finish_reason = 'end of text'\n",
    "        \n",
    "        return {\n",
    "            'all_tokens': all_tokens,\n",
    "            'total_text': new_text,\n",
    "            'finish_reason': finish_reason\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57b06d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:49:49.176619Z",
     "iopub.status.busy": "2024-02-21T15:49:49.175784Z",
     "iopub.status.idle": "2024-02-21T15:49:49.190083Z",
     "shell.execute_reply": "2024-02-21T15:49:49.189241Z",
     "shell.execute_reply.started": "2024-02-21T15:49:49.176582Z"
    }
   },
   "outputs": [],
   "source": [
    "text_generator = TextGenerator(model, \n",
    "                               tokenizer,\n",
    "                               context_size,\n",
    "                               tokenizer.token_to_id('[EOS]'), \n",
    "                               sample_top_p=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93751def",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:49:49.192040Z",
     "iopub.status.busy": "2024-02-21T15:49:49.191778Z",
     "iopub.status.idle": "2024-02-21T15:49:49.256334Z",
     "shell.execute_reply": "2024-02-21T15:49:49.255298Z",
     "shell.execute_reply.started": "2024-02-21T15:49:49.192016Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"Вот такие пироги\"\n",
    "text_generator.generate_token(text, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8acbff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:49:49.258588Z",
     "iopub.status.busy": "2024-02-21T15:49:49.257919Z",
     "iopub.status.idle": "2024-02-21T15:49:49.267349Z",
     "shell.execute_reply": "2024-02-21T15:49:49.266315Z",
     "shell.execute_reply.started": "2024-02-21T15:49:49.258557Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"В современном мире \"\n",
    "text_generator.generate_token(text, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050fad74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:49:49.268868Z",
     "iopub.status.busy": "2024-02-21T15:49:49.268571Z",
     "iopub.status.idle": "2024-02-21T15:49:49.385186Z",
     "shell.execute_reply": "2024-02-21T15:49:49.384263Z",
     "shell.execute_reply.started": "2024-02-21T15:49:49.268843Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"В современном мире \"\n",
    "text_generator.generate_text(text, max_tokens=64)['total_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986ea779",
   "metadata": {},
   "source": [
    "Модель достаточно простая - малый словарь, мало параметров к RNN, мало обучали, на малом количестве данных. Это приводит к тому, что тексты получаются не самые правдоподобные. Экспериментируя с гиперпараметрами (включая размер выбранного датасета), мне удалось достичь приемлемого качества генерации. Ради интереса, можете попробовать сделать также."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce87c6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
